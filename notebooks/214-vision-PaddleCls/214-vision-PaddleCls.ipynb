{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5206ff2e-8ee6-47b0-abef-eb1b7f9ec5ee",
   "metadata": {},
   "source": [
    "# PaddlePaddle Image Classification with OpenVINO\n",
    "This demo shows how to run MobileNetV3 Large PaddePaddle model on OpenVINO natively. Instead of exporting the PaddlePaddle model to ONNX and then create the Intermediate Representation (IR) format through OpenVINO optimizer, we can now read direct from the Paddle Model without any conversions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3dbb77-5cd6-46ad-b6e4-ba75f0ee795f",
   "metadata": {},
   "source": [
    "# Download the MobileNetV3_large_x1_0 Model\n",
    "Here we will direct the pre-trained model directly from the server. More details about the pretrained model can be found in PaddleClas documentation below.\n",
    "\n",
    "Source: https://github.com/PaddlePaddle/PaddleClas/blob/release/2.2/deploy/lite/readme_en.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d476b3-0d6e-420b-9e51-9a1629bda494",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import urllib.request\n",
    "import tarfile\n",
    "\n",
    "mobilenet_url = \"https://paddle-imagenet-models-name.bj.bcebos.com/dygraph/inference/MobileNetV3_large_x1_0_infer.tar\"\n",
    "mobilenetv3_model_path = \"model/MobileNetV3_large_x1_0_infer/inference.pdmodel\"\n",
    "if os.path.isfile(mobilenetv3_model_path): \n",
    "    print(\"Model MobileNetV3_large_x1_0 already existed\")\n",
    "else:\n",
    "    # Download the model from the server, and untar it.\n",
    "    print(\"Downloading the MobileNetV3_large_x1_0_infer model (20Mb)... May take a while...\")\n",
    "    # make the directory if it is not \n",
    "    os.makedirs('model')\n",
    "    urllib.request.urlretrieve(mobilenet_url, \"model/MobileNetV3_large_x1_0_infer.tar\")\n",
    "    print(\"Model Downloaded\")\n",
    "\n",
    "    file = tarfile.open(\"model/MobileNetV3_large_x1_0_infer.tar\")\n",
    "    res = file.extractall('model')\n",
    "    file.close()\n",
    "    if (not res):\n",
    "        print(\"Model Extracted to \\\"model/MobileNetV3_large_x1_0_infer\\\".\")\n",
    "    else:\n",
    "        print(\"Error Extracting the model. Please check the network.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a72a48-494e-44f3-9ee8-b2c15c845fdc",
   "metadata": {},
   "source": [
    "# Define the callback function for postprecessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b52bfa-551c-4d58-a796-7a0cb7e8c357",
   "metadata": {},
   "outputs": [],
   "source": [
    "def callback(infer_request, i) -> None:\n",
    "    imagenet_classes = json.loads(open(\"utils/imagenet_class_index.json\").read())\n",
    "    predictions = next(iter(infer_request.results.values()))\n",
    "    indices = np.argsort(-predictions[0])\n",
    "    if (i == 0):\n",
    "        # Calculate the first inference time\n",
    "        latency = time.time() - start\n",
    "        print(\"latency:\", + latency)\n",
    "        for i in range(5):\n",
    "            print(\n",
    "                \"Class name:\",\"'\" + imagenet_classes[str(list(indices)[i])][1] + \"'\",\n",
    "                \", probability:\" , predictions[0][list(indices)[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8adfba1-82d2-4526-910f-2d32530d74eb",
   "metadata": {},
   "source": [
    "# Read the model file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179ae1a7-966d-49fe-9c06-811257c57989",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openvino.runtime as ov\n",
    "\n",
    "core = ov.Core()\n",
    "# MobileNetV3_large_x1_0\n",
    "model = core.read_model(\"model/MobileNetV3_large_x1_0_infer/inference.pdmodel\")\n",
    "# get the information of intput and output layer\n",
    "input_layer = next(iter(model.inputs))\n",
    "output_layer = next(iter(model.outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6a1ee1-802b-4561-af31-a613bba10fc6",
   "metadata": {},
   "source": [
    "# Integrate prepocessing steps into execution graph with Preprocessing API\n",
    "When your input data don’t perfectly fit to Neural Network model input tensor - this means that additional operations/steps are needed to transform your data to format expected by model. These operations are known as “preprocessing”.\n",
    "Preprocessing steps will be integrated into execution graph and will be performed on selected device (CPU/GPU/VPU/etc.) rather than always being executed on CPU. This will improve selected device utilization which is always good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998aa8d0-20d7-471a-b4c5-25dd52c51881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from openvino.preprocess import PrePostProcessor\n",
    "from openvino.runtime import Layout, Type\n",
    "from openvino.preprocess import ResizeAlgorithm\n",
    "from openvino.runtime import AsyncInferQueue,PartialShape\n",
    "\n",
    "filename = \"coco.jpg\"\n",
    "test_image = cv2.imread(filename) \n",
    "test_image = np.expand_dims(test_image, 0) / 255\n",
    "_, h, w, _ = test_image.shape\n",
    "\n",
    "# Fix model’s input shape to get better performace\n",
    "model.reshape({input_layer.any_name: PartialShape([1,3,224,224])})\n",
    "ppp = PrePostProcessor(model)\n",
    "# Set input tensor information:\n",
    "# - input() provides information about a single model input\n",
    "# - layout of data is 'NHWC'\n",
    "# - set static spatial dimensions to input tensor to resize from\n",
    "ppp.input().tensor() \\\n",
    "    .set_spatial_static_shape(h,w) \\\n",
    "    .set_layout(Layout('NHWC')) \n",
    "inputs = model.inputs\n",
    "# Here we suppose model has 'NCHW' layout for input\n",
    "ppp.input().model().set_layout(Layout('NCHW'))\n",
    "# Do prepocessing:\n",
    "# - apply linear resize from tensor spatial dims to model spatial dims\n",
    "# - Subtract mean from each channel\n",
    "# - Divide each pixel data to appropriate scale value\n",
    "ppp.input().preprocess() \\\n",
    "    .resize(ResizeAlgorithm.RESIZE_LINEAR,224,224) \\\n",
    "    .mean([0.485, 0.456,0.406]) \\\n",
    "    .scale([0.229, 0.224, 0.225])\n",
    "# Set output tensor information:\n",
    "# - precision of tensor is supposed to be 'f32'\n",
    "ppp.output().tensor().set_element_type(Type.f32)\n",
    "# Apply preprocessing modifing the original 'model'\n",
    "model = ppp.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63289a21-3369-4f45-bd36-5f2e4f7aca01",
   "metadata": {},
   "source": [
    "# Run Inference\n",
    "Use “AUTO” as the device name to delegate selection of an actual accelerator to OpenVINO. The Auto-device plugin internally recognizes and selects devices from among CPU, integrated GPU and discrete Intel GPUs (when available) depending on the device capabilities and the characteristics of CNN models (for example, precision). Then the Auto-device assigns inference requests to the selected device.\n",
    "AUTO starts inferencing immediately on the CPU and then transparently shifts inferencing to the GPU (or VPU) once ready, dramatically reducing time to first inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f306c2-7589-4e0a-8de6-5bfb4b94ce0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython.display import Image\n",
    "import json\n",
    "\n",
    "# Check the available devices in your system\n",
    "devices = core.available_devices\n",
    "for device in devices:\n",
    "    device_name = core.get_property(device_name=device, name=\"FULL_DEVICE_NAME\")\n",
    "    print(f\"{device}: {device_name}\")\n",
    "\n",
    "# Loading model to a AUTO choosed device from the available devices list\n",
    "compiled_model = core.compile_model(model=model, device_name=\"AUTO\")\n",
    "# Create infer request queue\n",
    "infer_queue = AsyncInferQueue(compiled_model)\n",
    "infer_queue.set_callback(callback)\n",
    "start = time.time()\n",
    "# Do inference\n",
    "infer_queue.start_async({input_layer.any_name:test_image},0)\n",
    "infer_queue.wait_all()\n",
    "Image(filename=filename) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be337a5a-1e28-42cc-bb11-0fe9db5986c7",
   "metadata": {},
   "source": [
    "# Run Inference with \"LATENCY\" Performance Hint\n",
    "Expressing application target use-case with a single config key, letting the device configure itself to get a better \"LATENCY\" oriented performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5a5522-98ed-4bd5-8c1b-6296f31765c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUTO sets device config based on hints\n",
    "compiled_model = core.compile_model(model=model, device_name=\"AUTO\",config={\"PERFORMANCE_HINT\":\"LATENCY\"})\n",
    "infer_queue = AsyncInferQueue(compiled_model)\n",
    "infer_queue.set_callback(callback)\n",
    "start = time.time()\n",
    "for i in range(100):\n",
    "    infer_queue.start_async({input_layer.any_name:test_image},i)\n",
    "infer_queue.wait_all()\n",
    "end = time.time()\n",
    "# Calculate the average FPS\n",
    "fps = 100 / (end - start)\n",
    "print(\"fps:\", + fps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0807fd6b-16ac-4e90-b394-36a88a94abb6",
   "metadata": {},
   "source": [
    "# Run Inference with \"TRHOUGHPUT\" Performance Hint\n",
    "Expressing application target use-case with a single config key, letting the device configure itself to get a better \"THROUGHPUT\" oriented performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49997f5f-36d6-42b4-bce1-3eb44005fbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUTO sets device config based on hints\n",
    "compiled_model = core.compile_model(model=model, device_name=\"AUTO\",config={\"PERFORMANCE_HINT\":\"THROUGHPUT\"})\n",
    "infer_queue = AsyncInferQueue(compiled_model)\n",
    "infer_queue.set_callback(callback)\n",
    "start = time.time()\n",
    "for i in range(100):\n",
    "    infer_queue.start_async({input_layer.any_name:test_image},i)\n",
    "infer_queue.wait_all()\n",
    "end = time.time()\n",
    "# Calculate the average FPS\n",
    "fps = 100 / (end - start)\n",
    "print(\"fps:\", + fps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
