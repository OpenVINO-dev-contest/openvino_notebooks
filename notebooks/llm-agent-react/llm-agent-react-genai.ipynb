{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5fc61857-629d-476c-84fc-927a63a12f0f",
   "metadata": {},
   "source": [
    "# Create a native Agent using OpenVINO Generate API\n",
    "\n",
    "LLM are limited to the knowledge on which they have been trained and the additional knowledge provided as context, as a result, if a useful piece of information is missing the provided knowledge, the model cannot “go around” and try to find it in other sources. This is the reason why we need to introduce the concept of Agents.\n",
    "\n",
    "The core idea of agents is to use a language model to choose a sequence of actions to take. In agents, a language model is used as a reasoning engine to determine which actions to take and in which order. Agents can be seen as applications powered by LLMs and integrated with a set of tools like search engines, databases, websites, and so on. Within an agent, the LLM is the reasoning engine that, based on the user input, is able to plan and execute a set of actions that are needed to fulfill the request.\n",
    "\n",
    "![agent](https://github.com/openvinotoolkit/openvino_notebooks/assets/91237924/22fa5396-8381-400f-a78f-97e25d57d807)\n",
    "\n",
    "Previously, we already discussed how to build an instruction-following pipeline using OpenVINO, please check out [this tutorial](../llm-question-answering/llm-question-answering.ipynb) for reference.\n",
    "In this tutorial, we consider how to use the power of OpenVINO for running Large Language Models for chat. We will use a pre-trained model from the [Hugging Face Transformers](https://huggingface.co/docs/transformers/index) library. The [Hugging Face Optimum Intel](https://huggingface.co/docs/optimum/intel/index) library converts the models to OpenVINO™ IR format. To simplify the user experience, we will use [OpenVINO Generate API](https://github.com/openvinotoolkit/openvino.genai) for generation pipeline.\n",
    "\n",
    "#### Table of contents:\n",
    "\n",
    "- [Prerequisites](#Prerequisites)\n",
    "- [Create LLM as agent](#Create-LLM-as-agent)\n",
    "    - [Download model](#Select-model)\n",
    "    - [Select inference device for LLM](#Select-inference-device-for-LLM)\n",
    "    - [Instantiate pipeline with OpenVINO Generate API](#Instantiate-pipeline-with-OpenVINO-Generate-API)\n",
    "    - [Create text generation method](#Create-text-generation-method)\n",
    "- [Create prompt template](#Create-prompt-template)\n",
    "- [Create parser](#Create-parers)\n",
    "- [Create tools calling](#Create-tool-calling)\n",
    "- [Run agent](#Run-agent)\n",
    "- [Create AI agent demo with Gradio UI](#Create-AI-agent-demo-with-Gradio-UI)\n",
    "\n",
    "### Installation Instructions\n",
    "\n",
    "This is a self-contained example that relies solely on its own code.\n",
    "\n",
    "We recommend  running the notebook in a virtual environment. You only need a Jupyter server to start.\n",
    "For details, please refer to [Installation Guide](https://github.com/openvinotoolkit/openvino_notebooks/blob/latest/README.md#-installation-guide).\n",
    "\n",
    "<img referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=5b5a4db0-7875-4bfb-bdbd-01698b5b1a77&file=notebooks/llm-agent-react/llm-agent-rag-llamaindex.ipynb\" />\n",
    "\n",
    "<img referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=5b5a4db0-7875-4bfb-bdbd-01698b5b1a77&file=notebooks/llm-agent-react/llm-agent-react.ipynb\" />\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d6e66ddf",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd4d3ed6-01da-477f-a41b-0be41f758e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "llama-index-embeddings-openvino 0.5.0 requires huggingface-hub<0.24.0,>=0.23.0, but you have huggingface-hub 0.27.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "\n",
    "r = requests.get(\n",
    "    url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/notebook_utils.py\",\n",
    ")\n",
    "open(\"notebook_utils.py\", \"w\").write(r.text)\n",
    "\n",
    "r = requests.get(\n",
    "    url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/pip_helper.py\",\n",
    ")\n",
    "open(\"pip_helper.py\", \"w\").write(r.text)\n",
    "\n",
    "os.environ[\"GIT_CLONE_PROTECTION_ACTIVE\"] = \"false\"\n",
    "\n",
    "from pip_helper import pip_install\n",
    "\n",
    "pip_install(\n",
    "    \"-q\",\n",
    "    \"--extra-index-url\",\n",
    "    \"https://download.pytorch.org/whl/cpu\",\n",
    "    \"transformers>=4.43.1\",\n",
    "    \"gradio>=4.19\",\n",
    ")\n",
    "pip_install(\n",
    "    \"-q\",\n",
    "    \"git+https://github.com/huggingface/optimum-intel.git\",\n",
    "    \"git+https://github.com/openvinotoolkit/nncf.git\",\n",
    "    \"datasets\",\n",
    "    \"accelerate\",\n",
    "    \"huggingface-hub>=0.26.5\",\n",
    "    \"openvino-genai\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e04a1a2b-873b-4c05-a9fc-9a762ddeffe7",
   "metadata": {},
   "source": [
    "## Create LLM as agent\n",
    "\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "### Download LLM\n",
    "\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "To run LLM locally, we have to download the model in the first step. It is possible to [export your model](https://github.com/huggingface/optimum-intel?tab=readme-ov-file#export) to the OpenVINO IR format with the CLI, and load the model from local folder.\n",
    "\n",
    "Large Language Models (LLMs) are a core component of agent. LlamaIndex does not serve its own LLMs, but rather provides a standard interface for interacting with many different LLMs. In this example, we can select `Qwen2.5` as LLM in agent pipeline.\n",
    "* **qwen2.5-3b-instruct/qwen2.5-7b-instruct/qwen2.5-14b-instruct** - Qwen2.5 is the latest series of Qwen large language models. Comparing with Qwen2, Qwen2.5 series brings significant improvements in coding, mathematics and general knowledge skills. Additionally, it brings long-context and multiple languages support including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more. \n",
    "For more details, please refer to [model_card](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct), [blog](https://qwenlm.github.io/blog/qwen2.5/), [GitHub](https://github.com/QwenLM/Qwen2.5), and [Documentation](https://qwen.readthedocs.io/en/latest/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0c9e882-5622-454e-b28c-9d799678b8cd",
   "metadata": {
    "test_replace": {
     "Qwen/Qwen2.5-3B-Instruct": "Qwen/Qwen2.5-1.5B-Instruct"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e0f52db12c94c248bc070adbd3cf3ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Model:', options=('Qwen/Qwen2.5-3B-Instruct', 'Qwen/Qwen2.5-7B-Instruct', 'Qwen/qwen2.5-…"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "llm_model_ids = [\"Qwen/Qwen2.5-3B-Instruct\", \"Qwen/Qwen2.5-7B-Instruct\", \"Qwen/qwen2.5-14b-instruct\"]\n",
    "\n",
    "llm_model_id = widgets.Dropdown(\n",
    "    options=llm_model_ids,\n",
    "    value=llm_model_ids[0],\n",
    "    description=\"Model:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "llm_model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47707d27-9fe6-4bd6-b7c7-087311170a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "llm_model_path = llm_model_id.value.split(\"/\")[-1]\n",
    "\n",
    "if not Path(llm_model_path).exists():\n",
    "    !optimum-cli export openvino --model {llm_model_id.value} --task text-generation-with-past --trust-remote-code --weight-format int4 --group-size 128 --ratio 1.0 --sym {llm_model_path}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "30dcdf3a",
   "metadata": {},
   "source": [
    "### Select inference device for LLM\n",
    "\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74be1b36-a56d-41a7-9d1a-d487b22e5dd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4609595e6b8443690ae82c1e32e47e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Device:', options=('CPU', 'GPU', 'AUTO'), value='CPU')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from notebook_utils import device_widget\n",
    "\n",
    "llm_device = device_widget(\"CPU\", exclude=[\"NPU\"])\n",
    "\n",
    "llm_device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0e47dddf",
   "metadata": {},
   "source": [
    "## Instantiate pipeline with OpenVINO Generate API\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "[OpenVINO Generate API](https://github.com/openvinotoolkit/openvino.genai/blob/master/src/README.md) can be used to create pipelines to run an inference with OpenVINO Runtime. \n",
    "\n",
    "Firstly we need to create a pipeline with `LLMPipeline`. `LLMPipeline` is the main object used for text generation using LLM in OpenVINO GenAI API. You can construct it straight away from the folder with the converted model. We will provide directory with model and device for `LLMPipeline`. Then we run `generate` method and get the output in text format.\n",
    "Additionally, we can configure parameters for decoding. We can create the default config with `ov_genai.GenerationConfig()`, setup parameters, and apply the updated version with `set_generation_config(config)` or put config directly to `generate()`. It's also possible to specify the needed options just as inputs in the `generate()` method, as shown below, e.g. we can add `max_new_tokens` to stop generation if a specified number of tokens is generated and the end of generation is not reached. We will discuss some of the available generation parameters more deeply later.  Generation process for long response may be time consuming, for accessing partial result as soon as it is generated without waiting when whole process finished, Streaming API can be used. Token streaming is the mode in which the generative system returns the tokens one by one as the model generates them. This enables showing progressive generations to the user rather than waiting for the whole generation. Streaming is an essential aspect of the end-user experience as it reduces latency, one of the most critical aspects of a smooth experience. In code below, we implement simple streamer for printing output result. For more advanced streamer example please check openvino.genai [sample](https://github.com/openvinotoolkit/openvino.genai/tree/master/samples/python/multinomial_causal_lm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a6b3b57-e7bc-49a0-9812-bef7f7360a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openvino_genai\n",
    "\n",
    "pipe = openvino_genai.LLMPipeline(llm_model_path, llm_device.value)\n",
    "\n",
    "tokenizer = pipe.get_tokenizer()\n",
    "config = openvino_genai.GenerationConfig()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c2bff03",
   "metadata": {},
   "source": [
    "### Create text generation method\n",
    "\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "In this example, we would like to stream the output text though steamer, and stop text generation before `Observation` received from tool calling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16fc83b9-c78a-4601-b564-00845347f80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def streamer(subword):\n",
    "    print(subword, end=\"\", flush=True)\n",
    "    # Return flag corresponds whether generation should be stopped.\n",
    "    # False means continue generation.\n",
    "    return False\n",
    "\n",
    "\n",
    "def text_completion(prompt: str, stop_words) -> str:\n",
    "    im_end = \"<|im_end|>\"\n",
    "    if im_end not in stop_words:\n",
    "        stop_words = stop_words + [im_end]\n",
    "\n",
    "    config.max_new_tokens = 2000\n",
    "    config.top_k = 1\n",
    "    config.stop_strings = set(stop_words)\n",
    "    output = pipe.generate(prompt, config, streamer)\n",
    "    for stop_str in stop_words:\n",
    "        idx = output.find(stop_str)\n",
    "        if idx != -1:\n",
    "            output = output[: idx + len(stop_str)]\n",
    "    return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "05fe6e0c",
   "metadata": {},
   "source": [
    "## Create prompt template\n",
    "\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "A prompt for a language model is a set of instructions or input provided by a user to guide the model's response, helping it understand the context and generate relevant and coherent language-based output, such as answering questions, completing sentences, or engaging in a conversation.\n",
    "\n",
    "Different agents have different prompting styles for reasoning. In this example, we will use [ReAct agent](https://react-lm.github.io/) with its typical prompt template. For a full list of built-in agents see [agent types](https://python.langchain.com/docs/modules/agents/agent_types/).\n",
    "\n",
    "![react](https://github.com/user-attachments/assets/c26432c2-3cf1-4942-ae03-fd8e8ebb4509)\n",
    "\n",
    "A ReAct prompt consists of few-shot task-solving trajectories, with human-written text reasoning traces and actions, as well as environment observations in response to actions. ReAct prompting is intuitive and flexible to design, and achieves state-of-the-art few-shot performances across a variety of tasks, from question answering to online shopping!\n",
    "\n",
    "In an prompt template for agent, `query` is user's query and other parameter should be a sequence of messages that contains the `descriptions` and `parameters` of agent tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f614efa9-2c13-45ec-89ad-1dab63f346d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOOL_DESC = \"\"\"{name_for_model}: Call this tool to interact with the {name_for_human} API. What is the {name_for_human} API useful for? {description_for_model} Parameters: {parameters}\"\"\"\n",
    "\n",
    "PROMPT_REACT = \"\"\"Answer the following questions as best you can. You have access to the following APIs:\n",
    "\n",
    "{tools_text}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tools_name_text}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can be repeated zero or more times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {query}\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0548431e",
   "metadata": {},
   "source": [
    "Meanwhile we have to create function for consolidate the tools information and conversation history into the prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71d46914",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import json5\n",
    "\n",
    "\n",
    "def build_input_text(chat_history, list_of_tool_info) -> str:\n",
    "    tools_text = []\n",
    "    for tool_info in list_of_tool_info:\n",
    "        tool = TOOL_DESC.format(\n",
    "            name_for_model=tool_info[\"name_for_model\"],\n",
    "            name_for_human=tool_info[\"name_for_human\"],\n",
    "            description_for_model=tool_info[\"description_for_model\"],\n",
    "            parameters=json.dumps(tool_info[\"parameters\"], ensure_ascii=False),\n",
    "        )\n",
    "        if tool_info.get(\"args_format\", \"json\") == \"json\":\n",
    "            tool += \" Format the arguments as a JSON object.\"\n",
    "        elif tool_info[\"args_format\"] == \"code\":\n",
    "            tool += \" Enclose the code within triple backticks (`) at the beginning and end of the code.\"\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        tools_text.append(tool)\n",
    "    tools_text = \"\\n\\n\".join(tools_text)\n",
    "\n",
    "    tools_name_text = \", \".join([tool_info[\"name_for_model\"] for tool_info in list_of_tool_info])\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
    "    for i, (query, response) in enumerate(chat_history):\n",
    "        if list_of_tool_info:\n",
    "            if (len(chat_history) == 1) or (i == len(chat_history) - 2):\n",
    "                query = PROMPT_REACT.format(\n",
    "                    tools_text=tools_text,\n",
    "                    tools_name_text=tools_name_text,\n",
    "                    query=query,\n",
    "                )\n",
    "        if query:\n",
    "            messages.append({\"role\": \"user\", \"content\": query})\n",
    "        if response:\n",
    "            messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dbed3668",
   "metadata": {},
   "source": [
    "## Create parser\n",
    "\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "A Parser is used to convert raw output of LLM to the input arguments of tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c9cbc6a-6114-4183-b49d-25d0ae2b5a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_latest_tool_call(text):\n",
    "    tool_name, tool_args = \"\", \"\"\n",
    "    i = text.rfind(\"\\nAction:\")\n",
    "    j = text.rfind(\"\\nAction Input:\")\n",
    "    k = text.rfind(\"\\nObservation:\")\n",
    "    for stop_str in ['Observation\"}', \"Observation}\"]:\n",
    "        idx = text.find(stop_str)\n",
    "        if idx != -1:\n",
    "            text = text[:idx]\n",
    "    if 0 <= i < j:  # If the text has `Action` and `Action input`,\n",
    "        if k < j:  # but does not contain `Observation`,\n",
    "            # then it is likely that `Observation` is ommited by the LLM,\n",
    "            # because the output text may have discarded the stop word.\n",
    "            text = text.rstrip() + \"\\nObservation:\"  # Add it back.\n",
    "        k = text.rfind(\"\\nObservation:\")\n",
    "        tool_name = text[i + len(\"\\nAction:\") : j].strip()\n",
    "        tool_args = text[j + len(\"\\nAction Input:\") : k].strip()\n",
    "        text = text[:k]\n",
    "    return tool_name, tool_args, text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "23b1673b",
   "metadata": {},
   "source": [
    "## Create tools calling\n",
    "\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "In this examples, we will create 2 customized tools for `image generation` and `weather qurey`. A detailed description of these tools should be defined in json format, which will be used as part of prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b7b30a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"name_for_human\": \"get weather\",\n",
    "        \"name_for_model\": \"get_weather\",\n",
    "        \"description_for_model\": 'Get the current weather in a given city name.\"',\n",
    "        \"parameters\": [\n",
    "            {\n",
    "                \"name\": \"city_name\",\n",
    "                \"description\": \"City name\",\n",
    "                \"required\": True,\n",
    "                \"schema\": {\"type\": \"string\"},\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"name_for_human\": \"image generation\",\n",
    "        \"name_for_model\": \"image_gen\",\n",
    "        \"description_for_model\": \"AI painting (image generation) service, input text description, and return the image URL drawn based on text information.\",\n",
    "        \"parameters\": [\n",
    "            {\n",
    "                \"name\": \"prompt\",\n",
    "                \"description\": \"describe the image\",\n",
    "                \"required\": True,\n",
    "                \"schema\": {\"type\": \"string\"},\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "975b72e3",
   "metadata": {},
   "source": [
    "Then we should implement these tools with inputs and outputs, and execute them according to the output of LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b544d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_tool(tool_name: str, tool_args: str) -> str:\n",
    "    if tool_name == \"get_weather\":\n",
    "        city_name = json5.loads(tool_args)[\"city_name\"]\n",
    "        key_selection = {\n",
    "            \"current_condition\": [\n",
    "                \"temp_C\",\n",
    "                \"FeelsLikeC\",\n",
    "                \"humidity\",\n",
    "                \"weatherDesc\",\n",
    "                \"observation_time\",\n",
    "            ],\n",
    "        }\n",
    "        resp = requests.get(f\"https://wttr.in/{city_name}?format=j1\")\n",
    "        resp.raise_for_status()\n",
    "        resp = resp.json()\n",
    "        ret = {k: {_v: resp[k][0][_v] for _v in v} for k, v in key_selection.items()}\n",
    "        return str(ret)\n",
    "    elif tool_name == \"image_gen\":\n",
    "        import urllib.parse\n",
    "\n",
    "        tool_args = tool_args.replace(\"(\", \"\").replace(\")\", \"\")\n",
    "        prompt = json5.loads(tool_args)[\"prompt\"]\n",
    "        prompt = urllib.parse.quote(prompt)\n",
    "        return json.dumps(\n",
    "            {\"image_url\": f\"https://image.pollinations.ai/prompt/{prompt}\"},\n",
    "            ensure_ascii=False,\n",
    "        )\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "def llm_with_tool(prompt: str, history, list_of_tool_info=()):\n",
    "    chat_history = [(x[\"user\"], x[\"bot\"]) for x in history] + [(prompt, \"\")]\n",
    "\n",
    "    planning_prompt = build_input_text(chat_history, list_of_tool_info)\n",
    "    text = \"\"\n",
    "    while True:\n",
    "        output = text_completion(planning_prompt + text, stop_words=[\"Observation:\", \"Observation:\\n\"])\n",
    "        action, action_input, output = parse_latest_tool_call(output)\n",
    "        if action:\n",
    "            observation = call_tool(action, action_input)\n",
    "            output += f\"\\nObservation: {observation}\\nThought:\"\n",
    "            observation = f\"{observation}\\nThought:\"\n",
    "            print(observation)\n",
    "            text += output\n",
    "        else:\n",
    "            text += output\n",
    "            break\n",
    "\n",
    "    new_history = []\n",
    "    new_history.extend(history)\n",
    "    new_history.append({\"user\": prompt, \"bot\": text})\n",
    "    return text, new_history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "745969ca",
   "metadata": {},
   "source": [
    "## Run agent\n",
    "\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "560cd122-17b8-4d04-a75b-3d048ac89ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought: I should use the get_weather API to get the weather information for London and then use the image_gen API to create a picture of Big Ben based on that information.\n",
      "Action: get_weather\n",
      "Action Input: {\"city_name\": \"London\"}\n",
      "Observation\"}\n",
      "{'current_condition': {'temp_C': '1', 'FeelsLikeC': '-2', 'humidity': '93', 'weatherDesc': [{'value': 'Overcast'}], 'observation_time': '06:29 AM'}}\n",
      "Thought:\n",
      " I should use the weather information to create a picture of Big Ben. The current temperature is 1 degree Celsius, it feels like -2 degrees Celsius, the humidity is 93%, and the weather description is Overcast.\n",
      "Action: image_gen\n",
      "Action Input: {\"prompt\": \"Big Ben in London, overcast weather, 1 degree Celsius, feels like -2 degrees Celsius, humidity 93%\"}\n",
      "Observation}\n",
      "{\"image_url\": \"https://image.pollinations.ai/prompt/Big%20Ben%20in%20London%2C%20overcast%20weather%2C%201%20degree%20Celsius%2C%20feels%20like%20-2%20degrees%20Celsius%2C%20humidity%2093%25\"}\n",
      "Thought:\n",
      " I now have the image URL for the picture of Big Ben based on the weather information.\n",
      "Final Answer: The current weather in London is overcast with a temperature of 1 degree Celsius and feels like -2 degrees Celsius with a humidity of 93%. Here is the picture of Big Ben in London based on this weather information: ![](https://image.pollinations.ai/prompt/Big%20Ben%20in%20London%2C%20overcast%20weather%2C%201%20degree%20Celsius%2C%20feels%20like%20-2%20degrees%20Celsius%2C%20humidity%2093%25)"
     ]
    }
   ],
   "source": [
    "history = []\n",
    "query = \"get the weather in London, and create a picture of Big Ben based on the weather information\"\n",
    "\n",
    "response, history = llm_with_tool(prompt=query, history=history, list_of_tool_info=tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7999a8-32a7-48f2-a870-9b51ec47f92f",
   "metadata": {},
   "source": [
    "## Create AI agent demo with Gradio UI\n",
    "\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "555bef67-0101-4448-993f-692946d3a93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread, Event\n",
    "import queue\n",
    "\n",
    "\n",
    "class IterableStreamer(openvino_genai.StreamerBase):\n",
    "    \"\"\"\n",
    "    A custom streamer class for handling token streaming and detokenization with buffering.\n",
    "\n",
    "    Attributes:\n",
    "        tokenizer (Tokenizer): The tokenizer used for encoding and decoding tokens.\n",
    "        tokens_cache (list): A buffer to accumulate tokens for detokenization.\n",
    "        text_queue (Queue): A synchronized queue for storing decoded text chunks.\n",
    "        print_len (int): The length of the printed text to manage incremental decoding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer):\n",
    "        \"\"\"\n",
    "        Initializes the IterableStreamer with the given tokenizer.\n",
    "\n",
    "        Args:\n",
    "            tokenizer (Tokenizer): The tokenizer to use for encoding and decoding tokens.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokens_cache = []\n",
    "        self.text_queue = queue.Queue()\n",
    "        self.print_len = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Returns the iterator object itself.\n",
    "        \"\"\"\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        \"\"\"\n",
    "        Returns the next value from the text queue.\n",
    "\n",
    "        Returns:\n",
    "            str: The next decoded text chunk.\n",
    "\n",
    "        Raises:\n",
    "            StopIteration: If there are no more elements in the queue.\n",
    "        \"\"\"\n",
    "        value = self.text_queue.get()  # get() will be blocked until a token is available.\n",
    "        if value is None:\n",
    "            raise StopIteration\n",
    "        return value\n",
    "\n",
    "    def get_stop_flag(self):\n",
    "        \"\"\"\n",
    "        Checks whether the generation process should be stopped.\n",
    "\n",
    "        Returns:\n",
    "            bool: Always returns False in this implementation.\n",
    "        \"\"\"\n",
    "        return False\n",
    "\n",
    "    def put_word(self, word: str):\n",
    "        \"\"\"\n",
    "        Puts a word into the text queue.\n",
    "\n",
    "        Args:\n",
    "            word (str): The word to put into the queue.\n",
    "        \"\"\"\n",
    "        self.text_queue.put(word)\n",
    "\n",
    "    def put(self, token_id: int) -> bool:\n",
    "        \"\"\"\n",
    "        Processes a token and manages the decoding buffer. Adds decoded text to the queue.\n",
    "\n",
    "        Args:\n",
    "            token_id (int): The token_id to process.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if generation should be stopped, False otherwise.\n",
    "        \"\"\"\n",
    "        self.tokens_cache.append(token_id)\n",
    "        text = self.tokenizer.decode(self.tokens_cache)\n",
    "\n",
    "        word = \"\"\n",
    "        if len(text) > self.print_len and \"\\n\" == text[-1]:\n",
    "            # Flush the cache after the new line symbol.\n",
    "            word = text[self.print_len :]\n",
    "            self.tokens_cache = []\n",
    "            self.print_len = 0\n",
    "        elif len(text) >= 3 and text[-3:] == chr(65533):\n",
    "            # Don't print incomplete text.\n",
    "            pass\n",
    "        elif len(text) > self.print_len:\n",
    "            # It is possible to have a shorter text after adding new token.\n",
    "            # Print to output only if text length is increaesed.\n",
    "            word = text[self.print_len :]\n",
    "            self.print_len = len(text)\n",
    "        self.put_word(word)\n",
    "\n",
    "        if self.get_stop_flag():\n",
    "            # When generation is stopped from streamer then end is not called, need to call it here manually.\n",
    "            self.end()\n",
    "            return True  # True means stop  generation\n",
    "        else:\n",
    "            return False  # False means continue generation\n",
    "\n",
    "    def end(self):\n",
    "        \"\"\"\n",
    "        Flushes residual tokens from the buffer and puts a None value in the queue to signal the end.\n",
    "        \"\"\"\n",
    "        text = self.tokenizer.decode(self.tokens_cache)\n",
    "        if len(text) > self.print_len:\n",
    "            word = text[self.print_len :]\n",
    "            self.put_word(word)\n",
    "            self.tokens_cache = []\n",
    "            self.print_len = 0\n",
    "        self.put_word(None)\n",
    "\n",
    "    def reset(self):\n",
    "        self.tokens_cache = []\n",
    "        self.text_queue = queue.Queue()\n",
    "        self.print_len = 0\n",
    "\n",
    "\n",
    "class ChunkStreamer(IterableStreamer):\n",
    "\n",
    "    def __init__(self, tokenizer, tokens_len=4):\n",
    "        super().__init__(tokenizer)\n",
    "        self.tokens_len = tokens_len\n",
    "\n",
    "    def put(self, token_id: int) -> bool:\n",
    "        if (len(self.tokens_cache) + 1) % self.tokens_len != 0:\n",
    "            self.tokens_cache.append(token_id)\n",
    "            return False\n",
    "        return super().put(token_id)\n",
    "\n",
    "\n",
    "def run_chatbot(history):\n",
    "    \"\"\"\n",
    "    callback function for running chatbot on submit button click\n",
    "\n",
    "    Params:\n",
    "      history: conversation history\n",
    "\n",
    "    \"\"\"\n",
    "    chat_history = [(history[-1][0], \"\")]\n",
    "\n",
    "    prompt = build_input_text(chat_history, tools)\n",
    "    text = \"\"\n",
    "    while True:\n",
    "        planning_prompt = prompt + text\n",
    "        im_end = \"<|im_end|>\"\n",
    "        stop_words = [\"Observation:\", \"Observation:\\n\"]\n",
    "        if im_end not in stop_words:\n",
    "            stop_words = stop_words + [im_end]\n",
    "        streamer = ChunkStreamer(pipe.get_tokenizer())\n",
    "        config = openvino_genai.GenerationConfig()\n",
    "        config.max_new_tokens = 2000\n",
    "        config.stop_strings = set(stop_words)\n",
    "\n",
    "        stream_complete = Event()\n",
    "\n",
    "        def generate_and_signal_complete():\n",
    "            \"\"\"\n",
    "            genration function for single thread\n",
    "            \"\"\"\n",
    "            streamer.reset()\n",
    "            pipe.generate(planning_prompt, config, streamer)\n",
    "            stream_complete.set()\n",
    "            streamer.end()\n",
    "\n",
    "        t1 = Thread(target=generate_and_signal_complete)\n",
    "        t1.start()\n",
    "\n",
    "        output = \"\"\n",
    "        output_gui = \"\"\n",
    "        show_response = False\n",
    "        for new_text in streamer:\n",
    "            output += new_text\n",
    "            if \"Final\" in new_text:\n",
    "                show_response = True\n",
    "                idx = new_text.find(\"Final\")\n",
    "                new_text = new_text[idx:]\n",
    "            if show_response:\n",
    "                output_gui += new_text\n",
    "                history[-1][1] = output_gui\n",
    "                yield history\n",
    "\n",
    "        # assert buffer.startswith(prompt)\n",
    "        for stop_str in stop_words:\n",
    "            idx = output.find(stop_str)\n",
    "            if idx != -1:\n",
    "                output = output[: idx + len(stop_str)]\n",
    "        print(output)\n",
    "        action, action_input, output = parse_latest_tool_call(output)\n",
    "        if action:\n",
    "            observation = call_tool(action, action_input)\n",
    "            output += f\"\\nObservation: = {observation}\\nThought:\"\n",
    "            observation = f\"{observation}\\nThought:\"\n",
    "            print(observation)\n",
    "            text += output\n",
    "        else:\n",
    "            text += output\n",
    "            break\n",
    "\n",
    "\n",
    "def stop(streamer):\n",
    "    if streamer is not None:\n",
    "        streamer.end()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24d3a8e-58fa-4bc0-be7e-78a6054fcdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not Path(\"gradio_helper.py\").exists():\n",
    "    r = requests.get(url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/notebooks/llm-agent-react/gradio_helper.py\")\n",
    "    open(\"gradio_helper.py\", \"w\").write(r.text)\n",
    "\n",
    "from gradio_helper import make_demo\n",
    "\n",
    "examples = [\n",
    "    [\"Based on current weather in Beijing, show me a picture of Great Wall through its URL\"],\n",
    "    [\"Create an image of pink cat and return its URL\"],\n",
    "    [\"What is the weather like in New York now ?\"],\n",
    "]\n",
    "\n",
    "demo = make_demo(run_fn=run_chatbot, stop_fn=stop, examples=examples)\n",
    "\n",
    "try:\n",
    "    demo.launch()\n",
    "except Exception:\n",
    "    demo.launch(share=True)\n",
    "# If you are launching remotely, specify server_name and server_port\n",
    "# EXAMPLE: `demo.launch(server_name='your server name', server_port='server port in int')`\n",
    "# To learn more please refer to the Gradio docs: https://gradio.app/docs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "efdbf24f-20de-48f1-a5c1-5f381ffe1a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 5612\n"
     ]
    }
   ],
   "source": [
    "# please uncomment and run this cell for stopping gradio interface\n",
    "# demo.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "openvino_notebooks": {
   "imageUrl": "https://github.com/openvinotoolkit/openvino_notebooks/assets/91237924/22fa5396-8381-400f-a78f-97e25d57d807",
   "tags": {
    "categories": [
     "Model Demos",
     "AI Trends"
    ],
    "libraries": [],
    "other": [
     "LLM"
    ],
    "tasks": [
     "Text Generation"
    ]
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
