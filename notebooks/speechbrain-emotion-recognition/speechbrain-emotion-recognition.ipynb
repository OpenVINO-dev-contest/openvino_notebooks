{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpeechBrain Emotion Recognition with OpenVINO\n",
    "\n",
    "\n",
    "[SpeechBrain](https://github.com/speechbrain/speechbrain) is an open-source PyTorch toolkit that accelerates Conversational AI development, i.e., the technology behind speech assistants, chatbots, and large language models. \n",
    "\n",
    "Lear more in [GitHub repo](https://github.com/speechbrain/speechbrain) and [paper](https://arxiv.org/pdf/2106.04624)\n",
    "\n",
    "This notebook tutorial demonstrates optimization and inference of speechbrain emotion recognition model with OpenVINO.\n",
    "\n",
    "#### Table of contents:\n",
    "\n",
    "- [Installations](#Installations)\n",
    "- [Imports](#Imports)\n",
    "- [Prepare base model](#Prepare-base-model)\n",
    "- [Initialize model](#Initialize-model)\n",
    "- [PyTorch inference](#PyTorch-inference)\n",
    "- [SpeechBrain model optimization with Intel OpenVINO](#SpeechBrain-model-optimization-with-Intel-OpenVINO)\n",
    "    - [Step 1: Prepare input tensor](#Step-1:-Prepare-input-tensor)\n",
    "    - [Step 2: Convert model to OpenVINO IR](#Step-2:-Convert-model-to-OpenVINO-IR)\n",
    "    - [Step 3: OpenVINO model inference](#Step-3:-OpenVINO-model-inference)\n",
    "\n",
    "\n",
    "### Installation Instructions\n",
    "\n",
    "This is a self-contained example that relies solely on its own code.\n",
    "\n",
    "We recommend  running the notebook in a virtual environment. You only need a Jupyter server to start.\n",
    "For details, please refer to [Installation Guide](https://github.com/openvinotoolkit/openvino_notebooks/blob/latest/README.md#-installation-guide).\n",
    "\n",
    "<img referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=5b5a4db0-7875-4bfb-bdbd-01698b5b1a77&file=notebooks/speechbrain-emotion-recognition/speechbrain-emotion-recognition.ipynb\" />\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installations\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q \"speechbrain>=1.0.0\" --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "%pip install -q --upgrade --force-reinstall torch torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "%pip install -q \"transformers>=4.30.0\" \"huggingface_hub>=0.8.0\" \"SoundFile\"\n",
    "%pip install -q \"openvino>=2024.1.0\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from speechbrain.inference.interfaces import foreign_class\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "import openvino as ov"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare base model\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The foreign_class function in SpeechBrain is a utility that allows you to load and use custom PyTorch models within the SpeechBrain ecosystem. It provides a convenient way to integrate external or custom-built models into SpeechBrain's inference pipeline without modifying the core SpeechBrain codebase.\n",
    "\n",
    "1. source: This argument specifies the source or location of the pre-trained model checkpoint. In this case, \"speechbrain/emotion-recognition-wav2vec2-IEMOCAP\" refers to a pre-trained model checkpoint available on the Hugging Face Hub.\n",
    "2. pymodule_file: This argument is the path to a Python file containing the definition of your custom PyTorch model class. In this example, \"custom_interface.py\" is the name of the Python file that defines the CustomEncoderWav2vec2Classifier class.\n",
    "3. classname: This argument specifies the name of the custom PyTorch model class defined in the pymodule_file. In this case, \"CustomEncoderWav2vec2Classifier\" is the name of the class that extends SpeechBrain's Pretrained class and implements the necessary methods for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:speechbrain.utils.fetching:Fetch hyperparams.yaml: Fetching from HuggingFace Hub 'speechbrain/emotion-recognition-wav2vec2-IEMOCAP' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch custom_interface.py: Fetching from HuggingFace Hub 'speechbrain/emotion-recognition-wav2vec2-IEMOCAP' if not cached\n",
      "2024-11-05 19:56:33.843411: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-05 19:56:33.855243: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1730822193.869707   99152 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1730822193.873697   99152 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-05 19:56:33.888531: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39d5d4b05c5d45d58958fda13d211e3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.84k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ea/work/py311/lib/python3.11/site-packages/transformers/configuration_utils.py:306: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dcfad70be8a4718aeb5d547d99c924f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/380M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:speechbrain.lobes.models.huggingface_transformers.huggingface:speechbrain.lobes.models.huggingface_transformers.huggingface - Wav2Vec2Model is frozen.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac603a1816814b44a5d4562d6c86fab2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/159 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:speechbrain.utils.fetching:Fetch wav2vec2.ckpt: Fetching from HuggingFace Hub 'speechbrain/emotion-recognition-wav2vec2-IEMOCAP' if not cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c534ea7b8d5455089eda2f028218b83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wav2vec2.ckpt:   0%|          | 0.00/378M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a4d3adcce9747cf8d4a4e8a7ba3e533",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/380M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:speechbrain.utils.fetching:Fetch model.ckpt: Fetching from HuggingFace Hub 'speechbrain/emotion-recognition-wav2vec2-IEMOCAP' if not cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed691347a1db496a8143e133c261d3a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.ckpt:   0%|          | 0.00/13.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:speechbrain.utils.fetching:Fetch label_encoder.txt: Fetching from HuggingFace Hub 'speechbrain/emotion-recognition-wav2vec2-IEMOCAP' if not cached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d31fdf80b764b9bbf0091df95397292",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "label_encoder.txt:   0%|          | 0.00/83.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:speechbrain.utils.parameter_transfer:Loading pretrained files for: wav2vec2, model, label_encoder\n",
      "/home/ea/work/py311/lib/python3.11/site-packages/speechbrain/utils/checkpoints.py:200: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(path, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "classifier = foreign_class(\n",
    "    source=\"speechbrain/emotion-recognition-wav2vec2-IEMOCAP\", pymodule_file=\"custom_interface.py\", classname=\"CustomEncoderWav2vec2Classifier\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize model\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wav2vec2 torch model\n",
    "torch_model = classifier.mods[\"wav2vec2\"].model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch inference \n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform emotion recognition on the sample audio file.\n",
    "\n",
    "1. out_prob: Tensor or list containing the predicted probabilities or log probabilities for each emotion class.\n",
    "2. score: Scalar value representing the predicted probability or log probability of the most likely emotion class.\n",
    "3. index: Integer value representing the index of the most likely emotion class in the out_prob tensor or list.\n",
    "4. text_lab: String or list of strings containing the textual labels corresponding to the predicted emotion classes ([\"anger\", \"happiness\", \"sadness\", \"neutrality\"]). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c4fa6ee39374b28b571721dcbbc03a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "anger.wav:   0%|          | 0.00/201k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:speechbrain.dataio.encoder:CategoricalEncoder.expect_len was never called: assuming category count of 4 to be correct! Sanity check your encoder using `.expect_len`. Ensure that downstream code also uses the correct size. If you are sure this does not apply to you, use `.ignore_len`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion Recognition with SpeechBrain PyTorch model: ['ang']\n"
     ]
    }
   ],
   "source": [
    "hf_hub_download(repo_id=\"speechbrain/emotion-recognition-wav2vec2-IEMOCAP\", filename=\"anger.wav\", local_dir=\"data\")\n",
    "\n",
    "\n",
    "out_prob, score, index, text_lab = classifier.classify_file(\"data/anger.wav\")\n",
    "print(f\"Emotion Recognition with SpeechBrain PyTorch model: {text_lab}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SpeechBrain model optimization with Intel OpenVINO\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Prepare input tensor\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using sample audio file\n",
    "signals = []\n",
    "batch_size = 1\n",
    "signal, sr = torchaudio.load(str(\"data/anger.wav\"), channels_first=False)\n",
    "norm_audio = classifier.audio_normalizer(signal, sr)\n",
    "signals.append(norm_audio)\n",
    "\n",
    "sequence_length = norm_audio.shape[-1]\n",
    "\n",
    "wavs = torch.stack(signals, dim=0)\n",
    "wav_len = torch.tensor([sequence_length] * batch_size).unsqueeze(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Convert model to OpenVINO IR\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ea/work/py311/lib/python3.11/site-packages/transformers/modeling_utils.py:5006: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
      "/home/ea/work/py311/lib/python3.11/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:872: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_output.size() != (bsz, self.num_heads, tgt_len, self.head_dim):\n"
     ]
    }
   ],
   "source": [
    "# Model optimization process\n",
    "input_tensor = wavs.float()\n",
    "ov_model = ov.convert_model(torch_model, example_input=input_tensor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: OpenVINO model inference\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "305d3af26f884ac58fc431913151210a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Device:', index=1, options=('CPU', 'AUTO'), value='AUTO')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "if not Path(\"notebook_utils.py\").exists():\n",
    "    r = requests.get(\n",
    "        url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/notebook_utils.py\",\n",
    "    )\n",
    "    open(\"notebook_utils.py\", \"w\").write(r.text)\n",
    "\n",
    "from notebook_utils import device_widget\n",
    "\n",
    "device = device_widget()\n",
    "\n",
    "device\n",
    "\n",
    "# Read more about telemetry collection at https://github.com/openvinotoolkit/openvino_notebooks?tab=readme-ov-file#-telemetry\n",
    "from notebook_utils import collect_telemetry\n",
    "\n",
    "collect_telemetry(\"speechbrain-emotion-recognition.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion Recognition with OpenVINO Model: ['ang']\n"
     ]
    }
   ],
   "source": [
    "core = ov.Core()\n",
    "\n",
    "# OpenVINO Compiled model\n",
    "compiled_model = core.compile_model(ov_model, device.value)\n",
    "\n",
    "# Perform model inference\n",
    "output_tensor = compiled_model(wavs)[0]\n",
    "output_tensor = torch.from_numpy(output_tensor)\n",
    "\n",
    "# output post-processing\n",
    "outputs = classifier.mods.avg_pool(output_tensor, wav_len)\n",
    "outputs = outputs.view(outputs.shape[0], -1)\n",
    "outputs = classifier.mods.output_mlp(outputs).squeeze(1)\n",
    "ov_out_prob = classifier.hparams.softmax(outputs)\n",
    "score, index = torch.max(ov_out_prob, dim=-1)\n",
    "text_lab = classifier.hparams.label_encoder.decode_torch(index)\n",
    "\n",
    "print(f\"Emotion Recognition with OpenVINO Model: {text_lab}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "openvino_notebooks": {
   "imageUrl": "https://github.com/openvinotoolkit/openvino_notebooks/assets/29454499/727848d6-02a9-4259-ad48-8492879867a1",
   "tags": {
    "categories": [
     "Optimize"
    ],
    "libraries": [],
    "other": [],
    "tasks": [
     "Audio Classification"
    ]
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0430792a9b4d40e181ac16727a03aef2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "043180b8d0794fe99ddb4a01edddb5b1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "057203386f874cfb87759f5d00bba2a7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "05b70d85786248429e94502550b6486e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "0d26999f4c5d408baf746813083a1ff5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "1178d7aef62748d2a04cb667c6211b09": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_1baecd03768b4e69bd7c81e53a1e66b1",
       "style": "IPY_MODEL_d1fa098d18e642c7bd4898dfc93443a2",
       "value": " 1.84k/1.84k [00:00&lt;00:00, 174kB/s]"
      }
     },
     "1247d8fe0f2441c08ec197b256a5123b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "165c007b1b4044c2a7effd46c9ab6bc9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "18618e9c785442f38457f58f21672128": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_5d7b28cfd7fe4acbb8e0fe88f10b4920",
       "style": "IPY_MODEL_0430792a9b4d40e181ac16727a03aef2",
       "value": " 13.2k/13.2k [00:00&lt;00:00, 876kB/s]"
      }
     },
     "1baecd03768b4e69bd7c81e53a1e66b1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "1c4fa6ee39374b28b571721dcbbc03a8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_68c0b84e62534db6ab69c23c22c836d5",
        "IPY_MODEL_a59786be3c194e3f83242c70e86f616a",
        "IPY_MODEL_f4c0b47e6169406faaa0a795942a61ac"
       ],
       "layout": "IPY_MODEL_73bc992f17694aa19081274e8ca55a48"
      }
     },
     "252e7fa035c148d5a0ab9ebe1514aac0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "29a9bf0ff29648f58d647ba33e029fd3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "2a4d3adcce9747cf8d4a4e8a7ba3e533": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_49bd7700b7f943bc91542960712a6f71",
        "IPY_MODEL_cacb2c69f1f24d09981e663b57bca298",
        "IPY_MODEL_54f0839f5bae408794b1272136a47808"
       ],
       "layout": "IPY_MODEL_720f1c6d598640aaa3fbd72a28c7c4a1"
      }
     },
     "2b4523cb6418485897b11f8c2602e9da": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "2c674c2e4a2640b29c6cc341e10415c2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "2fe5e07c896343509a04983a8ece83c5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "305d3af26f884ac58fc431913151210a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DropdownModel",
      "state": {
       "_options_labels": [
        "CPU",
        "AUTO"
       ],
       "description": "Device:",
       "index": 1,
       "layout": "IPY_MODEL_ab55b1a8ef784b6a88b3f1b6e76f8d4e",
       "style": "IPY_MODEL_81d1b702695e402ca4358f6197a34205"
      }
     },
     "31f1eda54d6f42a0a7a1045d2166f96b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "39d5d4b05c5d45d58958fda13d211e3e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_c7f2f02d9db6494abd67c10b06789305",
        "IPY_MODEL_c6a007eea87a49cf8216148efc0d36a9",
        "IPY_MODEL_1178d7aef62748d2a04cb667c6211b09"
       ],
       "layout": "IPY_MODEL_65ed7f22327d46aeb86ec7938d7a6d5e"
      }
     },
     "3d31fdf80b764b9bbf0091df95397292": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_3d7a4330a54848b9998c43c11653e61a",
        "IPY_MODEL_53599eb05b44455cafb8897eeffc4739",
        "IPY_MODEL_72b337a604464e43baf7b23e40a559c6"
       ],
       "layout": "IPY_MODEL_90a46e122bc04c5dbdf11515c6778154"
      }
     },
     "3d7a4330a54848b9998c43c11653e61a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_d19f961b079c441694e5a87056316314",
       "style": "IPY_MODEL_31f1eda54d6f42a0a7a1045d2166f96b",
       "value": "label_encoder.txt: 100%"
      }
     },
     "41158f0d546044c1b4421e9f1dd0b0f9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "41277677a3af4441a890b3c879687477": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "4528e180e0a7432db1c07d74a8c8f3f5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "4858c992ff3e493ea9ca177d6d137fce": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "49bd7700b7f943bc91542960712a6f71": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_f5d3db494b554e2691830c850d6c566c",
       "style": "IPY_MODEL_165c007b1b4044c2a7effd46c9ab6bc9",
       "value": "model.safetensors: 100%"
      }
     },
     "4c17c6efb4ff4ddfa139538f55c04d41": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_b8314ad7d359499e8c5cfc327720081a",
       "max": 377575130,
       "style": "IPY_MODEL_9f7a203cacfb4e66a184052efb115e37",
       "value": 377575130
      }
     },
     "4c534ea7b8d5455089eda2f028218b83": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_5297380c39bb4acea03df2fe455b0522",
        "IPY_MODEL_4c17c6efb4ff4ddfa139538f55c04d41",
        "IPY_MODEL_ac14864e126a408d8fee16c028ca2e1e"
       ],
       "layout": "IPY_MODEL_be47130e34254df2af47c1c9bf2e15ee"
      }
     },
     "4ca7a49d85f249fea51ad78318a78947": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_b4311078580f409ca66343e08872fa93",
       "style": "IPY_MODEL_5bb8d670eab349d6a98fc0824463a1e7",
       "value": " 380M/380M [00:26&lt;00:00, 15.5MB/s]"
      }
     },
     "510cf565220647b1962f0341deacd058": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "5297380c39bb4acea03df2fe455b0522": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_e1bb471ab162435fad528d3e1fdc7725",
       "style": "IPY_MODEL_29a9bf0ff29648f58d647ba33e029fd3",
       "value": "wav2vec2.ckpt: 100%"
      }
     },
     "53599eb05b44455cafb8897eeffc4739": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_1247d8fe0f2441c08ec197b256a5123b",
       "max": 83,
       "style": "IPY_MODEL_d016333429754674a4fe69da318f83ef",
       "value": 83
      }
     },
     "54f0839f5bae408794b1272136a47808": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_2b4523cb6418485897b11f8c2602e9da",
       "style": "IPY_MODEL_05b70d85786248429e94502550b6486e",
       "value": " 380M/380M [00:27&lt;00:00, 14.7MB/s]"
      }
     },
     "57851edbbb26427bb9c05a3dd6be98a9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "5bb8d670eab349d6a98fc0824463a1e7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "5d7b28cfd7fe4acbb8e0fe88f10b4920": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "635be7dcd2d5496a8cefa9b15fae34a6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_c59ed05841874411bf3f4998a2ebe1e8",
       "max": 380267417,
       "style": "IPY_MODEL_2c674c2e4a2640b29c6cc341e10415c2",
       "value": 380267417
      }
     },
     "65ed7f22327d46aeb86ec7938d7a6d5e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "68c0b84e62534db6ab69c23c22c836d5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_4858c992ff3e493ea9ca177d6d137fce",
       "style": "IPY_MODEL_88adc2b82b2847fbb0278232f676bf0c",
       "value": "anger.wav: 100%"
      }
     },
     "720f1c6d598640aaa3fbd72a28c7c4a1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "72b337a604464e43baf7b23e40a559c6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_d7c9ccdbeb6942a78bf7a2ae9d72a9a5",
       "style": "IPY_MODEL_aca07b0a21e74b968c16129e1b4db2c0",
       "value": " 83.0/83.0 [00:00&lt;00:00, 10.7kB/s]"
      }
     },
     "73bc992f17694aa19081274e8ca55a48": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "777f12400348446a9091e5059fd9a8db": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "78bcb122aa4a4595ac8329d51400d53c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "80c4e421c72f4c4dbefc4e8103452782": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "81d1b702695e402ca4358f6197a34205": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "88adc2b82b2847fbb0278232f676bf0c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "894997a9f1d3476b8223d51a51de5d5c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "8f0d7f14a4b941498e380e1e28515907": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_80c4e421c72f4c4dbefc4e8103452782",
       "style": "IPY_MODEL_e9d0e8486a274b74b792f4a0acb55dd2",
       "value": "model.ckpt: 100%"
      }
     },
     "90a46e122bc04c5dbdf11515c6778154": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "9572bb009e464068bfcd0c91fdf21ffc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "96bbd928633348daba5490bf24c8924a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_2fe5e07c896343509a04983a8ece83c5",
       "style": "IPY_MODEL_cc29a4f36b5b484c948ca56aeee9675a",
       "value": "preprocessor_config.json: 100%"
      }
     },
     "9dcfad70be8a4718aeb5d547d99c924f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_c27fc80e8275466a815e2cb9a6eb162c",
        "IPY_MODEL_635be7dcd2d5496a8cefa9b15fae34a6",
        "IPY_MODEL_4ca7a49d85f249fea51ad78318a78947"
       ],
       "layout": "IPY_MODEL_41158f0d546044c1b4421e9f1dd0b0f9"
      }
     },
     "9f7a203cacfb4e66a184052efb115e37": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "a2813b573132490f90f977561d2c0e2b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_f30995627fce4743aca2b6f486a7f775",
       "max": 13176,
       "style": "IPY_MODEL_a4304c26b575459b9ec56c8363918363",
       "value": 13176
      }
     },
     "a4304c26b575459b9ec56c8363918363": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "a59786be3c194e3f83242c70e86f616a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_4528e180e0a7432db1c07d74a8c8f3f5",
       "max": 201110,
       "style": "IPY_MODEL_c6b9ecc9dbdf4726a29d27652d88b633",
       "value": 201110
      }
     },
     "ab55b1a8ef784b6a88b3f1b6e76f8d4e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ac14864e126a408d8fee16c028ca2e1e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_b2276f112ae44992b9cd39631069050a",
       "style": "IPY_MODEL_9572bb009e464068bfcd0c91fdf21ffc",
       "value": " 378M/378M [00:24&lt;00:00, 15.6MB/s]"
      }
     },
     "ac603a1816814b44a5d4562d6c86fab2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_96bbd928633348daba5490bf24c8924a",
        "IPY_MODEL_bdb84308147b41b08631c09fd718cbef",
        "IPY_MODEL_e327d5a16f5041cfbd03e97369c8177c"
       ],
       "layout": "IPY_MODEL_057203386f874cfb87759f5d00bba2a7"
      }
     },
     "aca07b0a21e74b968c16129e1b4db2c0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "af068b7cdcf7493992beb779397b7ea9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b2276f112ae44992b9cd39631069050a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b40ea5f2dab946278846e1821ae1a6de": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "b4311078580f409ca66343e08872fa93": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b8314ad7d359499e8c5cfc327720081a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "bdb84308147b41b08631c09fd718cbef": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_be54d91b796c42d685b02bc71eab5f96",
       "max": 159,
       "style": "IPY_MODEL_41277677a3af4441a890b3c879687477",
       "value": 159
      }
     },
     "be47130e34254df2af47c1c9bf2e15ee": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "be54d91b796c42d685b02bc71eab5f96": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c27fc80e8275466a815e2cb9a6eb162c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_777f12400348446a9091e5059fd9a8db",
       "style": "IPY_MODEL_510cf565220647b1962f0341deacd058",
       "value": "pytorch_model.bin: 100%"
      }
     },
     "c59ed05841874411bf3f4998a2ebe1e8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c6a007eea87a49cf8216148efc0d36a9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_d47f599f25e743a2b2e97257a5b6549c",
       "max": 1842,
       "style": "IPY_MODEL_b40ea5f2dab946278846e1821ae1a6de",
       "value": 1842
      }
     },
     "c6b9ecc9dbdf4726a29d27652d88b633": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "c7f2f02d9db6494abd67c10b06789305": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_78bcb122aa4a4595ac8329d51400d53c",
       "style": "IPY_MODEL_f29cf5b3919040519503227984d3a60e",
       "value": "config.json: 100%"
      }
     },
     "cacb2c69f1f24d09981e663b57bca298": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_252e7fa035c148d5a0ab9ebe1514aac0",
       "max": 380204696,
       "style": "IPY_MODEL_57851edbbb26427bb9c05a3dd6be98a9",
       "value": 380204696
      }
     },
     "cc29a4f36b5b484c948ca56aeee9675a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d016333429754674a4fe69da318f83ef": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "d19f961b079c441694e5a87056316314": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d1fa098d18e642c7bd4898dfc93443a2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d47f599f25e743a2b2e97257a5b6549c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d7c9ccdbeb6942a78bf7a2ae9d72a9a5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e1bb471ab162435fad528d3e1fdc7725": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e327d5a16f5041cfbd03e97369c8177c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_0d26999f4c5d408baf746813083a1ff5",
       "style": "IPY_MODEL_af068b7cdcf7493992beb779397b7ea9",
       "value": " 159/159 [00:00&lt;00:00, 16.8kB/s]"
      }
     },
     "e9d0e8486a274b74b792f4a0acb55dd2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ed691347a1db496a8143e133c261d3a8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_8f0d7f14a4b941498e380e1e28515907",
        "IPY_MODEL_a2813b573132490f90f977561d2c0e2b",
        "IPY_MODEL_18618e9c785442f38457f58f21672128"
       ],
       "layout": "IPY_MODEL_043180b8d0794fe99ddb4a01edddb5b1"
      }
     },
     "f29cf5b3919040519503227984d3a60e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f30995627fce4743aca2b6f486a7f775": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f4c0b47e6169406faaa0a795942a61ac": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_894997a9f1d3476b8223d51a51de5d5c",
       "style": "IPY_MODEL_f86ef249e623467d9889e84b8a5dfaed",
       "value": " 201k/201k [00:00&lt;00:00, 356kB/s]"
      }
     },
     "f5d3db494b554e2691830c850d6c566c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f86ef249e623467d9889e84b8a5dfaed": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
