{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40dac2c8-0544-4c62-8b41-bcfed22efffa",
   "metadata": {},
   "source": [
    "# PaddlePaddle Sentiment Classification with OpenVINO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ccc641-9aaf-4df7-a999-c5c17bc530fd",
   "metadata": {},
   "source": [
    "This simple demo shows how to run bert-base-uncased PaddePaddle model on OpenVINO natively. Instead of exporting the PaddlePaddle model to ONNX and then create the Intermediate Representation (IR) format through OpenVINO optimizer, we can now read direct from the Paddle Model without any conversions.\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers) applies Transformers, a popular attention model, to language modelling. This mechanism has an encoder to read the input text and a decoder that produces a prediction for the task. This model uses the technique of masking out some of the words in the input and then condition each word bidirectionally to predict the masked words. BERT also learns to model relationships between sentences, predicts if the sentences are connected or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5170a7-d930-443e-98b8-5ff96bc08f63",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f134de36-f5a4-4ac4-91d8-d1d58304656f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "from paddlenlp.data import Tuple, Pad\n",
    "from paddlenlp.transformers import BertTokenizer\n",
    "from openvino.runtime import Core"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfafacb",
   "metadata": {},
   "source": [
    "## Download PaddlePaddle BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b97d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def callbackfunc(blocknum, blocksize, totalsize):\n",
    "    '''Callback function to show the download progress\n",
    "    @blocknum: the number of data block which has been downloaded \n",
    "    @blocksize: the size of data blocks\n",
    "    @totalsize: the total size of a remote file\n",
    "    '''\n",
    "    percent = 100.0 * blocknum * blocksize / totalsize\n",
    "    if percent > 100:\n",
    "        percent = 100\n",
    "    sys.stdout.write('Download Progress：{:.5f}%\\r'.format(percent))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "bert_url = \"https://github.com/OpenVINO-dev-contest/models/raw/main/models/bert_sst.zip\"\n",
    "bert_model_path = Path(\"model/content/PaddleNLP/examples/language_model/bert/infer_model/model.pdmodel\")\n",
    "if bert_model_path.is_file(): \n",
    "    print(\"Model bert-base-uncased already exists\")\n",
    "else:\n",
    "    # Download the model from the server, and untar it.\n",
    "    print(\"Downloading the bert-base-uncased (400Mb)... May take a while...\")\n",
    "    # create a directory \n",
    "    os.makedirs(\"model\")\n",
    "    urllib.request.urlretrieve(bert_url, \"model/bert_sst2.zip\", callbackfunc)\n",
    "    print(\"Model Downloaded\")\n",
    "    try:\n",
    "        with zipfile.ZipFile(\"model/bert_sst2.zip\") as f:\n",
    "            f.extractall(\"./model\") \n",
    "        print(f\"Model Extracted to {bert_model_path}.\")\n",
    "    except Exception as e:\n",
    "        print(\"Error Extracting the model, type is:%s\" %type(e))\n",
    "    finally:\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb5516c-fb45-4068-a872-9b30a0908b33",
   "metadata": {},
   "source": [
    "## Convert_examples_to_features method\n",
    "Define the convert_examples_to_features method to get parameters from the input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998f775e-abed-48d7-b6e6-5132656fdc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the convert_examples_to_features method to get parameters from the input \n",
    "def convert_example(text, tokenizer,  max_seq_length=128):\n",
    "    \"\"\"\n",
    "    Define the convert_examples_to_features method to get parameters from the input\n",
    "    \n",
    "    :param: text: a list of input sentance\n",
    "            tokenizer: the pretrained tokenizers object\n",
    "            max_seq_length: the maximum length of a sentance can be loaded into inference engine, a over-sized sentance will be clustered \n",
    "    :retuns:\n",
    "            input_ids: token embedding\n",
    "            segment_ids: segment embedding\n",
    "    \"\"\"\n",
    "    encoded_inputs = tokenizer(text=text, max_seq_len=max_seq_length)\n",
    "    input_ids = encoded_inputs[\"input_ids\"]\n",
    "    segment_ids = encoded_inputs[\"token_type_ids\"]\n",
    "\n",
    "    return input_ids, segment_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7016a0-20d8-4bb6-9a11-a47ae8ffa3e8",
   "metadata": {},
   "source": [
    "## Define a predicter with OpenVINO\n",
    "Here we can read direct from the Paddle Model without any conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36231b56-a262-4714-a2a2-f2097dce4ec6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(model_path, max_seq_length, data, label_map, batch_size=1):\n",
    "    \"\"\"\n",
    "    Define a predicter with OpenVINO\n",
    "    \n",
    "    :param: model_path: the path of .pdmodel\n",
    "            max_seq_length: the maximum length of a sentance can be loaded into inference engine, a over-sized sentance will be clustered\n",
    "            data: a list of input sentance\n",
    "            label_map: a dict of label\n",
    "            batch_size: batch_size\n",
    "    :retuns:\n",
    "            outputs: Negative and Positive probablity\n",
    "            results: The result label for each sentance\n",
    "    \"\"\"\n",
    "    ie = Core()\n",
    "    # Directly loading a Paddle format model\n",
    "    model = ie.read_model(model=model_path + \"model.pdmodel\")\n",
    "    compiled_model = ie.compile_model(model=model, device_name=\"CPU\")\n",
    "    request = compiled_model.create_infer_request()\n",
    "    output_layer = next(iter(model.outputs))\n",
    "    examples = []\n",
    "    # loading (construction and loading) pretrained tokenizers\n",
    "    tokenizer = BertTokenizer.from_pretrained(\n",
    "            os.path.dirname(model_path))\n",
    "    # Convert examples to features\n",
    "    for text in data:\n",
    "        input_ids, segment_ids = convert_example(\n",
    "            text,\n",
    "            tokenizer,\n",
    "            max_seq_length)\n",
    "        examples.append((input_ids, segment_ids))\n",
    "    # Pads the input data samples to the largest length at `axis`\n",
    "    batchify_fn = lambda samples, fn=Tuple(\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id, dtype=\"int64\"),  # input\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id, dtype=\"int64\"),  # segment\n",
    "    ): fn(samples)\n",
    "    # Seperates data into some batches.\n",
    "    batches = [\n",
    "        examples[idx:idx + batch_size]\n",
    "        for idx in range(0, len(examples), batch_size)\n",
    "    ]\n",
    "    outputs = []\n",
    "    results = []\n",
    "    for batch in batches:\n",
    "        input_ids, segment_ids = batchify_fn(batch)\n",
    "        # Do inference, the input data is in dynamic shape\n",
    "        request.infer(inputs={model.inputs[1].any_name: input_ids, model.inputs[0].any_name: segment_ids})\n",
    "        result = request.get_output_tensor(output_layer.index).data\n",
    "        # Postprocessing to get the sentiment label and probility of each sentiment of each sentance\n",
    "        probs = softmax(result, axis=1)\n",
    "        idx = np.argmax(probs, axis=1)\n",
    "        idx = idx.tolist()\n",
    "        labels = [label_map[i] for i in idx]\n",
    "        outputs.extend(probs)\n",
    "        results.extend(labels)\n",
    "    return outputs, results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6514b671-8ae0-4e6f-88da-3b72896f7a1d",
   "metadata": {},
   "source": [
    "## Get the input data and do prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2616d5",
   "metadata": {},
   "source": [
    "The output results are the sentiment probablity of each input sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b9d5bb-abf3-4d95-97d0-f51995ed4aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    'OpenVINO accelerates applications with high-performance, AI and deep learning inference deployed from edge to cloud',\n",
    "    'The main disadvantage of padding is a bad performance due to spending time for processing dummy elements in the padding area',\n",
    "    'Model Optimizer adjusts deep learning models for optimal execution on end-point target devices',\n",
    "    'Reduce resource demands and efficiently deploy on a range of Intel® platforms from edge to cloud',\n",
    "    'It may have some accuracy drop'\n",
    "]\n",
    "label_map = {0: 'negative', 1: 'positive'}\n",
    "model_path = \"./model/\"\n",
    "max_seq_length = 128\n",
    "outputs, results = predict(model_path, max_seq_length, data, label_map)\n",
    "# Print the predicted label and probilitiy of each sentiment\n",
    "for idx, text in enumerate(data):\n",
    "    print(\n",
    "        'Data: {} \\n Label: {} \\n Negative prob: {} \\n Positive prob: {} \\n '.\n",
    "        format(text, results[idx], outputs[idx][0], outputs[idx][1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openvino_env",
   "language": "python",
   "name": "openvino_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
