{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2d0db251-6c4d-423d-a580-d883efe0708c",
   "metadata": {},
   "source": [
    "# Multimodal RAG for video analytics with LlamaIndex\n",
    "\n",
    "Constructing a RAG pipeline for text is relatively straightforward, thanks to the tools developed for parsing, indexing, and retrieving text data. However, adapting RAG models for video content presents a greater challenge. Videos combine visual, auditory, and textual elements, requiring more processing power and sophisticated video pipelines.\n",
    "\n",
    "To build a truly multimodal search for videos, you need to work with different modalities of a video like spoken content, visual. In this notebook, we showcase a Multimodal RAG pipeline designed for video analytics. It utilizes Whisper model to convert spoken content to text, CLIP model to generate multimodal embeddings, and Vision Language model (VLM) to process retrieved images and text messages. The following picture illustrates how this pipeline is working.\n",
    "\n",
    "![Multimodal RAG](https://github.com/user-attachments/assets/baef4914-5c07-432c-9363-1a0cb5944b09)\n",
    "\n",
    "#### Table of contents:\n",
    "\n",
    "- [Prerequisites](#Prerequisites)\n",
    "- [Convert and Compress models](#Convert-and-Compress-models)\n",
    "    - [ASR model](#ASR-model)\n",
    "    - [CLIP model](#CLIP-model)\n",
    "    - [VLM model](#VLM-model)\n",
    "- [Download and process video](#Download-and-process-video)\n",
    "    - [Initialize ASR](#Initialize-ASR)\n",
    "- [Create the multi-modal index](#Create-the-multi-modal-index)\n",
    "    - [Initialize CLIP](#Initialize-CLIP)\n",
    "- [Search text and image embeddings](#Search-text-and-image-embeddings)\n",
    "- [Generate final response using VLM](#Generate-final-response-using-VLM)\n",
    "    - [Set the RAG prompt template](#Set-the-RAG-prompt-template)\n",
    "    - [Initialize VLM](#Initialize-VLM)\n",
    "- [Interactive Demo](#Interactive-Demo)\n",
    "\n",
    "### Installation Instructions\n",
    "\n",
    "This is a self-contained example that relies solely on its own code.\n",
    "\n",
    "We recommend  running the notebook in a virtual environment. You only need a Jupyter server to start.\n",
    "For details, please refer to [Installation Guide](https://github.com/openvinotoolkit/openvino_notebooks/blob/latest/README.md#-installation-guide).\n",
    "\n",
    "\n",
    "<img referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=5b5a4db0-7875-4bfb-bdbd-01698b5b1a77&file=notebooks/multimodal-rag/multimodal-rag-llamaindex.ipynb\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d85cc2-6b0a-458c-9332-84a70f54ce86",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "install required packages and setup helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad6c48df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q \"llama-index-core\" \"llama-index-embeddings-openvino>=0.4.1\" \"llama-index-readers-file\" \\\n",
    "    \"llama-index-vector-stores-qdrant\"  \\\n",
    "    \"transformers==4.45\" \\\n",
    "    \"pytube\" \\\n",
    "    \"moviepy==1.0.3\" \\\n",
    "    \"yt-dlp\" \\\n",
    "    \"open_clip_torch\" \\\n",
    "    \"gradio>=4.44.1\" --extra-index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c61cb01-9c46-46e3-bf22-20c4ca0da417",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "os.environ[\"GIT_CLONE_PROTECTION_ACTIVE\"] = \"false\"\n",
    "\n",
    "if not Path(\"ov_phi3_vision_helper.py\").exists():\n",
    "    r = requests.get(url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/notebooks/phi-3-vision/ov_phi3_vision_helper.py\")\n",
    "    open(\"ov_phi3_vision_helper.py\", \"w\", encoding=\"utf-8\").write(r.text)\n",
    "\n",
    "if not Path(\"ov_phi3_llamaindex.py\").exists():\n",
    "    r = requests.get(url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/notebooks/multimodal-rag/ov_phi3_llamaindex.py\")\n",
    "    open(\"ov_phi3_llamaindex.py\", \"w\", encoding=\"utf-8\").write(r.text)\n",
    "    \n",
    "if not Path(\"notebook_utils.py\").exists():\n",
    "    r = requests.get(url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/notebook_utils.py\")\n",
    "    open(\"notebook_utils.py\", \"w\", encoding=\"utf-8\").write(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498b50cd-6b8f-4377-8705-1522af8de31a",
   "metadata": {},
   "source": [
    "## Convert and Compress models\n",
    "\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "### ASR model\n",
    "\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "In this example, we utilize [Distil-Whisper](https://huggingface.co/distil-whisper/distil-large-v2) to recognize the spoken content in video and generate text. Distil-Whisper is a distilled variant of the [Whisper](https://huggingface.co/openai/whisper-large-v2) model by OpenAI. The Distil-Whisper is proposed in the paper [Robust Knowledge Distillation via Large-Scale Pseudo Labelling](https://arxiv.org/abs/2311.00430). According to authors, compared to Whisper, Distil-Whisper runs in several times faster with 50% fewer parameters, while performing to within 1% word error rate (WER) on out-of-distribution evaluation data.\n",
    "For more information about Distil-Whisper, please refer [Distil-Whisper notebook](../distil-whisper-asr)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4d0e724",
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_model_id = \"distil-whisper/distil-large-v3\"\n",
    "asr_model_path = asr_model_id.split(\"/\")[-1]\n",
    "\n",
    "if not Path(asr_model_path).exists():\n",
    "    !optimum-cli export openvino --model {asr_model_id} {asr_model_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45424e39-7854-448a-9ab1-33331c5016d7",
   "metadata": {},
   "source": [
    "### CLIP model\n",
    "\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "In this example, CLIP model will help to generate the embedding vectors for both text and images. CLIP (Contrastive Language-Image Pre-Training) is a neural network trained on various (image, text) pairs. It can be instructed in natural language to predict the most relevant text snippet, given an image, without directly optimizing for the task.\n",
    "\n",
    "CLIP uses a [ViT](https://arxiv.org/abs/2010.11929) like transformer to get visual features and a causal language model to get the text features. The text and visual features are then projected into a latent space with identical dimensions. The dot product between the projected image and text features is then used as a similarity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2ea678c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model_id = \"laion/CLIP-ViT-B-32-laion2B-s34B-b79K\"\n",
    "clip_model_path = clip_model_id.split(\"/\")[-1]\n",
    "\n",
    "if not Path(clip_model_path).exists():\n",
    "    !optimum-cli export openvino -m {clip_model_id} {clip_model_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40053ea9-fb8c-4a03-af08-f4b7ac4b9e95",
   "metadata": {},
   "source": [
    "### VLM model\n",
    "\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Vision Language model (VLM) is used to generate final response regrading the context of images and texts retrieved from vector DB. It can help to understand the both language and image instructions to complete various real-world tasks. In this example, we select [Phi-3.5-Vision](https://huggingface.co/microsoft/Phi-3.5-vision-instruct) as VLM.\n",
    "\n",
    "The Phi-3-Vision is a lightweight, state-of-the-art open multimodal model built upon datasets which include - synthetic data and filtered publicly available websites - with a focus on very high-quality, reasoning dense data both on text and vision. The model belongs to the Phi-3 model family, and the multimodal version comes with 128K context length (in tokens) it can support. The model underwent a rigorous enhancement process, incorporating both supervised fine-tuning and direct preference optimization to ensure precise instruction adherence and robust safety measures. More details about model can be found in [model blog post](https://azure.microsoft.com/en-us/blog/new-models-added-to-the-phi-3-family-available-on-microsoft-azure/), [technical report](https://aka.ms/phi3-tech-report), [Phi-3-cookbook](https://github.com/microsoft/Phi-3CookBook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eedcf36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:NNCF initialized successfully. Supported frameworks detected: torch, onnx, openvino\n"
     ]
    }
   ],
   "source": [
    "from ov_phi3_vision_helper import convert_phi3_model\n",
    "import nncf\n",
    "\n",
    "vlm_model_id = \"microsoft/Phi-3.5-vision-instruct\"\n",
    "vlm_model_path = vlm_model_id.split(\"/\")[-1]\n",
    "if not Path(vlm_model_path).exists():\n",
    "    compression_configuration = {\n",
    "        \"mode\": nncf.CompressWeightsMode.INT4_SYM,\n",
    "        \"group_size\": 64,\n",
    "        \"ratio\": 0.6,\n",
    "        }\n",
    "    convert_phi3_model(vlm_model_id, vlm_model_path, compression_configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91b445b",
   "metadata": {},
   "source": [
    "## Download and process video\n",
    "\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "To begin, download an example video from YouTube and extract the audio and frame files from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "093464db-893e-4813-a6cc-19473a1a890c",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_url = \"https://www.youtube.com/watch?v=d_qvLDhkg00\"\n",
    "output_video_path = \"./video_data/\"\n",
    "output_folder = \"./mixed_data/\"\n",
    "output_audio_path = \"./mixed_data/output_audio.wav\"\n",
    "\n",
    "filepath = output_video_path + \"input_vid.mp4\"\n",
    "Path(output_folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if not Path(filepath).exists():\n",
    "    !yt-dlp {video_url} -f best -o {filepath}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d8f627",
   "metadata": {},
   "source": [
    "### Initialize ASR\n",
    "\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Select inference device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6636cd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c58ecdc195004a0893e1d8172d0f3259",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Device:', index=2, options=('CPU', 'GPU', 'AUTO'), value='AUTO')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from notebook_utils import device_widget\n",
    "\n",
    "asr_device = device_widget(default=\"AUTO\", exclude=[\"NPU\"])\n",
    "\n",
    "asr_device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f9b19a",
   "metadata": {},
   "source": [
    "The Hugging Face Optimum API is a high-level API that enables us to convert and quantize models from the Hugging Face Transformers library to the OpenVINO™ IR format. For more details, refer to the [Hugging Face Optimum documentation](https://huggingface.co/docs/optimum/intel/inference).\n",
    "\n",
    "Optimum Intel can be used to load optimized models from the [Hugging Face Hub](https://huggingface.co/docs/optimum/intel/hf.co/models) and create pipelines to run an inference with OpenVINO Runtime using Hugging Face APIs. The Optimum Inference models are API compatible with Hugging Face Transformers models.  This means we just need to replace the `AutoModelForXxx` class with the corresponding `OVModelForXxx` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "534c83b8-a8f4-499f-bfad-6799fdbabe8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.intel import OVModelForSpeechSeq2Seq\n",
    "from transformers import AutoProcessor, pipeline\n",
    "\n",
    "asr_model = OVModelForSpeechSeq2Seq.from_pretrained(asr_model_path, device=asr_device.value)\n",
    "asr_processor = AutoProcessor.from_pretrained(asr_model_path)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=asr_model,\n",
    "    tokenizer=asr_processor.tokenizer,\n",
    "    feature_extractor=asr_processor.feature_extractor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d300f17f-bf8d-4cc2-a61a-86fbb2529b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytube import YouTube\n",
    "from moviepy.video.io.VideoFileClip import VideoFileClip\n",
    "\n",
    "def download_video(url, output_path):\n",
    "    \"\"\"\n",
    "    Download a video from a given url and save it to the output path.\n",
    "\n",
    "    Params:\n",
    "    url (str): The url of the video to download.\n",
    "    output_path (str): The path to save the video to.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing the metadata of the video.\n",
    "    \"\"\"\n",
    "    yt = YouTube(url)\n",
    "    metadata = {\"Author\": yt.author, \"Title\": yt.title, \"Views\": yt.views}\n",
    "    yt.streams.get_highest_resolution().download(\n",
    "        output_path=output_path, filename=\"input_vid.mp4\"\n",
    "    )\n",
    "    return metadata\n",
    "\n",
    "\n",
    "def video_to_images(video_path, output_folder):\n",
    "    \"\"\"\n",
    "    Convert a video to a sequence of images and save them to the output folder.\n",
    "\n",
    "    Params:\n",
    "    video_path (str): The path to the video file.\n",
    "    output_folder (str): The path to the folder to save the images to.\n",
    "\n",
    "    \"\"\"\n",
    "    clip = VideoFileClip(video_path)\n",
    "    clip.write_images_sequence(\n",
    "        os.path.join(output_folder, \"frame%04d.png\"), fps=0.2\n",
    "    )\n",
    "\n",
    "\n",
    "def video_to_audio(video_path, output_audio_path):\n",
    "    \"\"\"\n",
    "    Convert a video to audio and save it to the output path.\n",
    "\n",
    "    Params:\n",
    "    video_path (str): The path to the video file.\n",
    "    output_audio_path (str): The path to save the audio to.\n",
    "\n",
    "    \"\"\"\n",
    "    clip = VideoFileClip(video_path)\n",
    "    audio = clip.audio\n",
    "    audio.write_audiofile(output_audio_path)\n",
    "\n",
    "\n",
    "def audio_to_text(audio_path):\n",
    "    \"\"\"\n",
    "    Convert audio to text using the SpeechRecognition library.\n",
    "\n",
    "    Params:\n",
    "    audio_path (str): The path to the audio file.\n",
    "\n",
    "    Returns:\n",
    "    test (str): The text recognized from the audio.\n",
    "\n",
    "    \"\"\"\n",
    "    result = pipe(audio_path, return_timestamps=True)\n",
    "\n",
    "\n",
    "    return result[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b1e544",
   "metadata": {},
   "source": [
    "In this step, we will extract the images and audio from video, then convert its audio into text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8f45641a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Writing frames ./mixed_data/frame%04d.png.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                                                                                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done writing frames ./mixed_data/frame%04d.png.\n",
      "MoviePy - Writing audio in ./mixed_data/output_audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                                                                                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text data saved to file\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    video_to_images(filepath, output_folder)\n",
    "    video_to_audio(filepath, output_audio_path)\n",
    "    text_data = audio_to_text(output_audio_path)\n",
    "    \n",
    "    with open(output_folder + \"output_text.txt\", \"w\") as file:\n",
    "        file.write(text_data)\n",
    "    print(\"Text data saved to file\")\n",
    "    file.close()\n",
    "    os.remove(output_audio_path)\n",
    "\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ec33e4",
   "metadata": {},
   "source": [
    "## Create the multi-modal index\n",
    "\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "In this step, we are going to build multi-modal index and vector store to index both text and images. The CLIP model is used to generate the embedding vector for texts and images.\n",
    "\n",
    "\n",
    "### Initialize CLIP\n",
    "\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Select inference device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "effcfe0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86c2ca09261545c1bc02fc266a6061ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Device:', index=2, options=('CPU', 'GPU', 'AUTO'), value='AUTO')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip_device = device_widget(default=\"AUTO\", exclude=[\"NPU\"])\n",
    "\n",
    "clip_device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73a4746",
   "metadata": {},
   "source": [
    "Class `OpenVINOClipEmbedding` in LlamaIndex can support exporting and loading open_clip models with OpenVINO runtime. for more information, please refer [Local Embeddings with OpenVINO](https://docs.llamaindex.ai/en/stable/examples/embeddings/openvino/#openclip-model-exporter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c2e7d376-4348-4810-9e5a-8237964a2aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface_openvino import OpenVINOClipEmbedding\n",
    "\n",
    "clip_model = OpenVINOClipEmbedding(model_id_or_path=clip_model_path, device=clip_device.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f106c225-0742-4f79-a089-c0534f589d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.indices import MultiModalVectorStoreIndex\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core import StorageContext, Settings\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "import qdrant_client\n",
    "\n",
    "# Create the MultiModal index\n",
    "documents = SimpleDirectoryReader(output_folder).load_data()\n",
    "\n",
    "# Create a local Qdrant vector store\n",
    "client = qdrant_client.QdrantClient(\":memory:\")\n",
    "\n",
    "text_store = QdrantVectorStore(\n",
    "    client=client, collection_name=\"text_collection\"\n",
    ")\n",
    "image_store = QdrantVectorStore(\n",
    "    client=client, collection_name=\"image_collection\"\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=text_store, image_store=image_store\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c74c3b59-3feb-449c-953c-f4d6ed522ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Payload indexes have no effect in the local Qdrant. Please use server Qdrant if you need payload indexes.\n",
      "WARNING:root:Payload indexes have no effect in the local Qdrant. Please use server Qdrant if you need payload indexes.\n"
     ]
    }
   ],
   "source": [
    "Settings.embed_model = clip_model\n",
    "\n",
    "index = MultiModalVectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    "    image_embed_model=Settings.embed_model,\n",
    "    transformations=[SentenceSplitter(chunk_size=500, chunk_overlap=50)]\n",
    ")\n",
    "\n",
    "retriever_engine = index.as_retriever(\n",
    "    similarity_top_k=2, image_similarity_top_k=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64fd2cf",
   "metadata": {},
   "source": [
    "## Search text and image embeddings\n",
    "\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "To simply the prompt for VLM, we have to prepare the context of text and images regarding user's query. In this step, the most relevant context will be retrieved from vector DB through multi-modal index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0ef0ebc2-a8f0-4dc6-90b6-2988e0ee20f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.response.notebook_utils import display_source_node\n",
    "from llama_index.core.schema import ImageNode\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def plot_images(image_paths):\n",
    "    images_shown = 0\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    for img_path in image_paths:\n",
    "        if os.path.isfile(img_path):\n",
    "            image = Image.open(img_path)\n",
    "\n",
    "            plt.subplot(2, 3, images_shown + 1)\n",
    "            plt.imshow(image)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "\n",
    "            images_shown += 1\n",
    "            if images_shown >= 7:\n",
    "                break\n",
    "\n",
    "def retrieve(retriever_engine, query_str):\n",
    "    retrieval_results = retriever_engine.retrieve(query_str)\n",
    "\n",
    "    retrieved_image = []\n",
    "    retrieved_text = []\n",
    "    for res_node in retrieval_results:\n",
    "        if isinstance(res_node.node, ImageNode):\n",
    "            retrieved_image.append(res_node.node.metadata[\"file_path\"])\n",
    "        else:\n",
    "            display_source_node(res_node, source_length=200)\n",
    "            retrieved_text.append(res_node.text)\n",
    "\n",
    "    return retrieved_image, retrieved_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7fad6de8-fac4-4df9-a544-94179d52b625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 3d226fc5-6556-4f4a-9bf3-582a3de27767<br>**Similarity:** 0.6292942845978948<br>**Text:** The basic. But the question. Of the gauceau's quare, egos. Of all, why, we're a theory. for the probability, and many of the same, the same, for the theory of the real-that move to make a symmetric...<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** a293ddaf-8be1-46c9-9210-a683e3ab2eac<br>**Similarity:** 0.605394755606113<br>**Text:** ...to, that the common between. What you're a conclusion that, is the convolution. ...toe goes to the conclusion. Sogous, is a convolution. is itself. If you, a mean, and two-toe for a new. If you,...<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"The basic. But the question. Of the gauceau's quare, egos. Of all, why, we're a theory. for the probability, and many of the same, the same, for the theory of the real-that move to make a symmetric. I'm in a symmetric. Ande to see a number number, I'm going, and a number, and then theery to see a number. For Thee. For, I'm still, and through the most, why I'm going, I'm trying to adder. For, I'm sorry, and a certain. We Haveene-teree. We have found out of course. We're to-d, weed graph, weed-d, I'd aided. Weir and a small. We had a bit moreed. We Did That You Add-ncrap, weed-d. We Are You Are To Bears, weed-d. We Are You Added, I'd die, I'm aided. Weed-weight. Weed-weight Thameshari, a little biterer, as many different, then, the same the central. The sum. The sum, then, the sum. The sum describing, tends to the same. And then, then the sum to the sum, and the sum that's a number. But the number. The number describing the system, tends to a number. I'm that's the same thing. The sum. that that number theory. The number theorem. The normal theorem is the central theorem. And then the number. Buter, that's thing that's. So that's a number theorem. What's. That's being a normal theorem. What's being, that's better thing. Ander is the sum. Buter is the sum. I'm theorem, and then the term. We're number theory, and the minimum. We're actually, we're different, the number. We're different. We're going. We're going variables.\", \"...to, that the common between. What you're a conclusion that, is the convolution. ...toe goes to the conclusion. Sogous, is a convolution. is itself. If you, a mean, and two-toe for a new. If you, essentially, you're a certain. of two and a normal distribution will be normal zero to, and a reintroduce, and a normal of two, and reintroduce, essentially, a convoluting. Essentially, and we were to a essentially, reintroduce. Essentially, and really to a number, andvolution. If you're at least, andir-stow, andrews, I'm going to a number. Isred, I'm current of course. Ite between, ...is, and you're andd, I'm aft. Wee andrews. If You Out of course. If You Are Thed the result is a bit of sigma. If You Are Thatd as a convolution. Is Is Not The Norr of something that's a brian. If You End-t the result. We Are Two. If You End-d that is aft. Is Is Another You Are That Is Is The Cur-tt-t of course you end up. If You End-lions. We're going to beware of course you end up. If You End Up-re. I'm a computer. If You All You Have You Have You Have You Have You Have You Have To-b-s. I'm from fisc-l-eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeu-luch. It's-luch's aunun-mult-it-alal. It's a few different kind of the way. So I'll see. Also, and you're a little for a... of the different. I'll do you'll, and so, and then I'm a sort of you're all, for the different to a different. Also, still, for a different. Also, still, for sure are, I'll make, for the different. Still.\"]\n"
     ]
    }
   ],
   "source": [
    "query_str = \"tell me more about gaussian function\"\n",
    "\n",
    "img, txt = retrieve(retriever_engine=retriever_engine, query_str=query_str)\n",
    "image_documents = SimpleDirectoryReader(\n",
    "    input_dir=output_folder, input_files=img\n",
    ").load_data()\n",
    "context_str = \"\".join(txt)\n",
    "plot_images(img)\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1d68d6",
   "metadata": {},
   "source": [
    "## Generate final response using VLM\n",
    "\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "### Set the RAG prompt template\n",
    "\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bb3eec2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_tmpl_str = (\n",
    "    \"Given the provided information, including relevant images and retrieved context from the video, \\\n",
    " accurately and precisely answer the query without any additional prior knowledge.\\n\"\n",
    "    \"Please ensure honesty and responsibility, refraining from any racist or sexist remarks.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Context: {context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0105ee1b",
   "metadata": {},
   "source": [
    "### Initialize VLM\n",
    "\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Select inference device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4737c584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b3899ea84704c12baa2b5c5df374c08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Device:', index=2, options=('CPU', 'GPU', 'AUTO'), value='AUTO')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vlm_device = device_widget(default=\"AUTO\", exclude=[\"NPU\"])\n",
    "\n",
    "vlm_device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344d6500",
   "metadata": {},
   "source": [
    "`OpenVINOPhi3MultiModal` class provides convenient way for running multimodal model in LlamaIndex. It accepts directory with converted model and inference device as arguments. For running model with streaming we will use `stream_complete` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9dfef4c6-92da-4997-b937-c73b6743ca04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ov_phi3_llamaindex import OpenVINOPhi3MultiModal\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    vlm_model_path, trust_remote_code=True\n",
    ")\n",
    "\n",
    "vlm = OpenVINOPhi3MultiModal(\n",
    "    model_id_or_path=vlm_model_path,\n",
    "    device=vlm_device.value,\n",
    "    max_new_tokens = 1024,\n",
    "    generate_kwargs={\"do_sample\": False, \"eos_token_id\" : processor.tokenizer.eos_token_id},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "56623614-0d44-49ac-bc9e-f8d87c659d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Gaussian function, also known as the normal distribution, is a probability distribution that is symmetric and bell-shaped. It is widely used in statistics and probability theory to model the distribution of random variables. The Gaussian function is defined by its mean and standard deviation, which determine the center and spread of the distribution. The probability density function of the Gaussian distribution is given by the equation:\n",
      "\n",
      "f(x) = (1/√(2πσ^2)) * e^(-(x-μ)^2 / (2σ^2))\n",
      "\n",
      "where μ is the mean and σ is the standard deviation. The Gaussian distribution is characterized by its tails approaching zero as x moves away from the mean, and its peak at the mean value. The area under the curve of the Gaussian distribution is equal to 1, representing the total probability of all possible outcomes. The Gaussian distribution is also known for its property of being fully described by its mean and standard deviation, making it a convenient and powerful tool for modeling and analyzing data."
     ]
    }
   ],
   "source": [
    "response = vlm.stream_complete(\n",
    "    prompt=qa_tmpl_str.format(\n",
    "        context_str=context_str, query_str=query_str\n",
    "    ),\n",
    "    image_documents=image_documents,\n",
    ")\n",
    "for r in response:\n",
    "    print(r.delta, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c92517c",
   "metadata": {},
   "source": [
    "## Interactive Demo\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Now, you can try to chat with model. Upload video, provide your text message into `Input` field and click `Submit` to start communication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e7d202",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def build_index(video_path):\n",
    "    \"\"\"\n",
    "    callback function for building index of vector store\n",
    "    \n",
    "    Params:\n",
    "      video_path: path of uploaded video file\n",
    "    Returns:\n",
    "      vector store is ready\n",
    "      \n",
    "    \"\"\"\n",
    "    \n",
    "    global retriever_engine\n",
    "    progress=gr.Progress()\n",
    "    progress(None, desc=\"Video to Images...\")\n",
    "    video_to_images(video_path, output_folder)\n",
    "    progress(None, desc=\"Video to Audio...\")\n",
    "    video_to_audio(video_path, output_audio_path)\n",
    "    progress(None, desc=\"Audio to Texts...\")\n",
    "    text_data = audio_to_text(output_audio_path)\n",
    "\n",
    "    with open(output_folder + \"output_text.txt\", \"w\") as file:\n",
    "        file.write(text_data)\n",
    "    print(\"Text data saved to file\")\n",
    "    file.close()\n",
    "    os.remove(output_audio_path)\n",
    "    \n",
    "    progress(0, desc=\"Building Index...\")\n",
    "    documents = SimpleDirectoryReader(output_folder).load_data()\n",
    "    client = qdrant_client.QdrantClient(\":memory:\")\n",
    "\n",
    "    text_store = QdrantVectorStore(\n",
    "        client=client, collection_name=\"text_collection\"\n",
    "    )\n",
    "    image_store = QdrantVectorStore(\n",
    "        client=client, collection_name=\"image_collection\"\n",
    "    )\n",
    "    storage_context = StorageContext.from_defaults(\n",
    "        vector_store=text_store, image_store=image_store\n",
    "    )\n",
    "    index = MultiModalVectorStoreIndex.from_documents(\n",
    "        documents,\n",
    "        storage_context=storage_context,\n",
    "        image_embed_model=Settings.embed_model,\n",
    "        transformations=[SentenceSplitter(chunk_size=500, chunk_overlap=50)]\n",
    "    )\n",
    "\n",
    "    retriever_engine = index.as_retriever(\n",
    "        similarity_top_k=2, image_similarity_top_k=5\n",
    "    )\n",
    "    return \"Vector Store is Ready\"\n",
    "    \n",
    "    \n",
    "def search(history):\n",
    "    \"\"\"\n",
    "    callback function for searching vector store\n",
    "\n",
    "    Params:\n",
    "      history: conversation history\n",
    "    Returns:\n",
    "      lists of retrieved images and texts\n",
    "      \n",
    "    \"\"\"\n",
    "    progress=gr.Progress()\n",
    "    progress(None, desc=\"Searching...\")\n",
    "    img, txt = retrieve(retriever_engine=retriever_engine, query_str=history[-1][0])\n",
    "    return img, txt \n",
    "\n",
    "def generate(history, images, texts)  :\n",
    "    \"\"\"\n",
    "    callback function for running chatbot on submit button click\n",
    "\n",
    "    Params:\n",
    "      history: conversation history\n",
    "      images: list of retrieved images\n",
    "      texts: list of retrieved texts\n",
    "\n",
    "    \"\"\"\n",
    "    progress=gr.Progress()\n",
    "    progress(None, desc=\"Generating...\")\n",
    "    history[-1][1] = gr.Gallery(images)\n",
    "    image_documents = SimpleDirectoryReader(\n",
    "        input_dir=output_folder, input_files=images\n",
    "    ).load_data()\n",
    "    context_str = \"\".join(texts)\n",
    "    \n",
    "    response = vlm.stream_complete(\n",
    "        prompt=qa_tmpl_str.format(\n",
    "            context_str=context_str, query_str=history[-1][0]\n",
    "        ),\n",
    "        image_documents=image_documents,\n",
    "    )\n",
    "    partial_text = \"\"\n",
    "    history = history + [[None, \"\"]]\n",
    "    for r in response:\n",
    "        partial_text += r.delta\n",
    "        history[-1][1] = partial_text\n",
    "        yield history\n",
    "\n",
    "\n",
    "def stop():\n",
    "    vlm._model.request.cancel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cb1cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not Path(\"gradio_helper.py\").exists():\n",
    "    r = requests.get(url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/notebooks/multimodal-rag/gradio_helper.py\")\n",
    "    open(\"gradio_helper.py\", \"w\").write(r.text)\n",
    "\n",
    "from gradio_helper import make_demo\n",
    "\n",
    "demo = make_demo(filepath, build_index, search, generate, stop)\n",
    "\n",
    "try:\n",
    "    demo.queue().launch()\n",
    "except Exception:\n",
    "    demo.queue().launch(share=True)\n",
    "# if you are launching remotely, specify server_name and server_port\n",
    "# demo.launch(server_name='your server name', server_port='server port in int')\n",
    "# Read more in the docs: https://gradio.app/docs/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "openvino_notebooks": {
   "imageUrl": "https://github.com/user-attachments/assets/baef4914-5c07-432c-9363-1a0cb5944b09",
   "tags": {
    "categories": [
     "Model Demos",
     "AI Trends"
    ],
    "libraries": [],
    "other": [],
    "tasks": [
     "Visual Question Answering",
     "Video-to-Text"
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
