{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80653d8c-3eb8-4a88-b23f-c31537e82b08",
   "metadata": {},
   "source": [
    "# LTX Video and OpenVINO™\n",
    "\n",
    "[LTX-Video](https://github.com/Lightricks/LTX-Video) is a transformer-based latent diffusion model that adopts a holistic approach to video generation by seamlessly integrating the responsibilities of the Video-VAE and the denoising transformer. Unlike existing methods, which treat these components as independent, LTX-Video aims to optimize their interaction for improved efficiency and quality. At its core is a carefully designed Video-VAE that achieves a high compression ratio of 1:192, with spatiotemporal downscaling of 32×32×8 pixels per token, enabled by relocating the patchifying operation from the transformer’s input to the VAE's input. Operating in this highly compressed latent space enables the transformer to efficiently perform full spatiotemporal selfattention, which is essential for generating high-resolution videos with temporal consistency. However, the high compression inherently limits the representation of fine details. To address this, this VAE decoder is tasked with both latent-to-pixel conversion and the final denoising step, producing the clean result directly in pixel space. This approach preserves the ability to generate fine details without incurring the runtime cost of a separate upsampling module. The model supports diverse use cases, including text-to-video and image-to-video generation, with both capabilities\n",
    "trained simultaneously.\n",
    "\n",
    "In this example we show how to convert text-to-video pipeline in OpenVINO format and run inference. For reducing memory consumption, weights compression optimization can be applied using [NNCF](https://github.com/openvinotoolkit/nncf).\n",
    "\n",
    "| | | | |\n",
    "|:---:|:---:|:---:|:---:|\n",
    "| ![example1](https://media.githubusercontent.com/media/Lightricks/LTX-Video/refs/heads/main/docs/_static/ltx-video_example_00001.gif)<br><details style=\"max-width: 300px; margin: auto;\"><summary>A woman with long brown hair and light skin smiles at another woman...</summary>A woman with long brown hair and light skin smiles at another woman with long blonde hair. The woman with brown hair wears a black jacket and has a small, barely noticeable mole on her right cheek. The camera angle is a close-up, focused on the woman with brown hair's face. The lighting is warm and natural, likely from the setting sun, casting a soft glow on the scene. The scene appears to be real-life footage.</details> | ![example2](https://media.githubusercontent.com/media/Lightricks/LTX-Video/refs/heads/main//docs/_static/ltx-video_example_00002.gif)<br><details style=\"max-width: 300px; margin: auto;\"><summary>A woman walks away from a white Jeep parked on a city street at night...</summary>A woman walks away from a white Jeep parked on a city street at night, then ascends a staircase and knocks on a door. The woman, wearing a dark jacket and jeans, walks away from the Jeep parked on the left side of the street, her back to the camera; she walks at a steady pace, her arms swinging slightly by her sides; the street is dimly lit, with streetlights casting pools of light on the wet pavement; a man in a dark jacket and jeans walks past the Jeep in the opposite direction; the camera follows the woman from behind as she walks up a set of stairs towards a building with a green door; she reaches the top of the stairs and turns left, continuing to walk towards the building; she reaches the door and knocks on it with her right hand; the camera remains stationary, focused on the doorway; the scene is captured in real-life footage.</details> | ![example3](https://media.githubusercontent.com/media/Lightricks/LTX-Video/refs/heads/main//docs/_static/ltx-video_example_00003.gif)<br><details style=\"max-width: 300px; margin: auto;\"><summary>A woman with blonde hair styled up, wearing a black dress...</summary>A woman with blonde hair styled up, wearing a black dress with sequins and pearl earrings, looks down with a sad expression on her face. The camera remains stationary, focused on the woman's face. The lighting is dim, casting soft shadows on her face. The scene appears to be from a movie or TV show.</details> | ![example4](https://media.githubusercontent.com/media/Lightricks/LTX-Video/refs/heads/main//docs/_static/ltx-video_example_00004.gif)<br><details style=\"max-width: 300px; margin: auto;\"><summary>The camera pans over a snow-covered mountain range...</summary>The camera pans over a snow-covered mountain range, revealing a vast expanse of snow-capped peaks and valleys.The mountains are covered in a thick layer of snow, with some areas appearing almost white while others have a slightly darker, almost grayish hue. The peaks are jagged and irregular, with some rising sharply into the sky while others are more rounded. The valleys are deep and narrow, with steep slopes that are also covered in snow. The trees in the foreground are mostly bare, with only a few leaves remaining on their branches. The sky is overcast, with thick clouds obscuring the sun. The overall impression is one of peace and tranquility, with the snow-covered mountains standing as a testament to the power and beauty of nature.</details> |\n",
    "| ![example5](https://media.githubusercontent.com/media/Lightricks/LTX-Video/refs/heads/main//docs/_static/ltx-video_example_00005.gif)<br><details style=\"max-width: 300px; margin: auto;\"><summary>A woman with light skin, wearing a blue jacket and a black hat...</summary>A woman with light skin, wearing a blue jacket and a black hat with a veil, looks down and to her right, then back up as she speaks; she has brown hair styled in an updo, light brown eyebrows, and is wearing a white collared shirt under her jacket; the camera remains stationary on her face as she speaks; the background is out of focus, but shows trees and people in period clothing; the scene is captured in real-life footage.</details> | ![example6](https://media.githubusercontent.com/media/Lightricks/LTX-Video/refs/heads/main//docs/_static/ltx-video_example_00006.gif)<br><details style=\"max-width: 300px; margin: auto;\"><summary>A man in a dimly lit room talks on a vintage telephone...</summary>A man in a dimly lit room talks on a vintage telephone, hangs up, and looks down with a sad expression. He holds the black rotary phone to his right ear with his right hand, his left hand holding a rocks glass with amber liquid. He wears a brown suit jacket over a white shirt, and a gold ring on his left ring finger. His short hair is neatly combed, and he has light skin with visible wrinkles around his eyes. The camera remains stationary, focused on his face and upper body. The room is dark, lit only by a warm light source off-screen to the left, casting shadows on the wall behind him. The scene appears to be from a movie.</details> | ![example7](https://media.githubusercontent.com/media/Lightricks/LTX-Video/refs/heads/main//docs/_static/ltx-video_example_00007.gif)<br><details style=\"max-width: 300px; margin: auto;\"><summary>A prison guard unlocks and opens a cell door...</summary>A prison guard unlocks and opens a cell door to reveal a young man sitting at a table with a woman. The guard, wearing a dark blue uniform with a badge on his left chest, unlocks the cell door with a key held in his right hand and pulls it open; he has short brown hair, light skin, and a neutral expression. The young man, wearing a black and white striped shirt, sits at a table covered with a white tablecloth, facing the woman; he has short brown hair, light skin, and a neutral expression. The woman, wearing a dark blue shirt, sits opposite the young man, her face turned towards him; she has short blonde hair and light skin. The camera remains stationary, capturing the scene from a medium distance, positioned slightly to the right of the guard. The room is dimly lit, with a single light fixture illuminating the table and the two figures. The walls are made of large, grey concrete blocks, and a metal door is visible in the background. The scene is captured in real-life footage.</details> | ![example8](https://media.githubusercontent.com/media/Lightricks/LTX-Video/refs/heads/main//docs/_static/ltx-video_example_00008.gif)<br><details style=\"max-width: 300px; margin: auto;\"><summary>A woman with blood on her face and a white tank top...</summary>A woman with blood on her face and a white tank top looks down and to her right, then back up as she speaks. She has dark hair pulled back, light skin, and her face and chest are covered in blood. The camera angle is a close-up, focused on the woman's face and upper torso. The lighting is dim and blue-toned, creating a somber and intense atmosphere. The scene appears to be from a movie or TV show.</details> |\n",
    "| ![example9](https://media.githubusercontent.com/media/Lightricks/LTX-Video/refs/heads/main//docs/_static/ltx-video_example_00009.gif)<br><details style=\"max-width: 300px; margin: auto;\"><summary>A man with graying hair, a beard, and a gray shirt...</summary>A man with graying hair, a beard, and a gray shirt looks down and to his right, then turns his head to the left. The camera angle is a close-up, focused on the man's face. The lighting is dim, with a greenish tint. The scene appears to be real-life footage. Step</details> | ![example10](https://media.githubusercontent.com/media/Lightricks/LTX-Video/refs/heads/main//docs/_static/ltx-video_example_00010.gif)<br><details style=\"max-width: 300px; margin: auto;\"><summary>A clear, turquoise river flows through a rocky canyon...</summary>A clear, turquoise river flows through a rocky canyon, cascading over a small waterfall and forming a pool of water at the bottom.The river is the main focus of the scene, with its clear water reflecting the surrounding trees and rocks. The canyon walls are steep and rocky, with some vegetation growing on them. The trees are mostly pine trees, with their green needles contrasting with the brown and gray rocks. The overall tone of the scene is one of peace and tranquility.</details> | ![example11](https://media.githubusercontent.com/media/Lightricks/LTX-Video/refs/heads/main//docs/_static/ltx-video_example_00011.gif)<br><details style=\"max-width: 300px; margin: auto;\"><summary>A man in a suit enters a room and speaks to two women...</summary>A man in a suit enters a room and speaks to two women sitting on a couch. The man, wearing a dark suit with a gold tie, enters the room from the left and walks towards the center of the frame. He has short gray hair, light skin, and a serious expression. He places his right hand on the back of a chair as he approaches the couch. Two women are seated on a light-colored couch in the background. The woman on the left wears a light blue sweater and has short blonde hair. The woman on the right wears a white sweater and has short blonde hair. The camera remains stationary, focusing on the man as he enters the room. The room is brightly lit, with warm tones reflecting off the walls and furniture. The scene appears to be from a film or television show.</details> | ![example12](https://media.githubusercontent.com/media/Lightricks/LTX-Video/refs/heads/main//docs/_static/ltx-video_example_00012.gif)<br><details style=\"max-width: 300px; margin: auto;\"><summary>The waves crash against the jagged rocks of the shoreline...</summary>The waves crash against the jagged rocks of the shoreline, sending spray high into the air.The rocks are a dark gray color, with sharp edges and deep crevices. The water is a clear blue-green, with white foam where the waves break against the rocks. The sky is a light gray, with a few white clouds dotting the horizon.</details> |\n",
    "| ![example13](https://media.githubusercontent.com/media/Lightricks/LTX-Video/refs/heads/main//docs/_static/ltx-video_example_00013.gif)<br><details style=\"max-width: 300px; margin: auto;\"><summary>The camera pans across a cityscape of tall buildings...</summary>The camera pans across a cityscape of tall buildings with a circular building in the center. The camera moves from left to right, showing the tops of the buildings and the circular building in the center. The buildings are various shades of gray and white, and the circular building has a green roof. The camera angle is high, looking down at the city. The lighting is bright, with the sun shining from the upper left, casting shadows from the buildings. The scene is computer-generated imagery.</details> | ![example14](https://media.githubusercontent.com/media/Lightricks/LTX-Video/refs/heads/main//docs/_static/ltx-video_example_00014.gif)<br><details style=\"max-width: 300px; margin: auto;\"><summary>A man walks towards a window, looks out, and then turns around...</summary>A man walks towards a window, looks out, and then turns around. He has short, dark hair, dark skin, and is wearing a brown coat over a red and gray scarf. He walks from left to right towards a window, his gaze fixed on something outside. The camera follows him from behind at a medium distance. The room is brightly lit, with white walls and a large window covered by a white curtain. As he approaches the window, he turns his head slightly to the left, then back to the right. He then turns his entire body to the right, facing the window. The camera remains stationary as he stands in front of the window. The scene is captured in real-life footage.</details> | ![example15](https://media.githubusercontent.com/media/Lightricks/LTX-Video/refs/heads/main//docs/_static/ltx-video_example_00015.gif)<br><details style=\"max-width: 300px; margin: auto;\"><summary>Two police officers in dark blue uniforms and matching hats...</summary>Two police officers in dark blue uniforms and matching hats enter a dimly lit room through a doorway on the left side of the frame. The first officer, with short brown hair and a mustache, steps inside first, followed by his partner, who has a shaved head and a goatee. Both officers have serious expressions and maintain a steady pace as they move deeper into the room. The camera remains stationary, capturing them from a slightly low angle as they enter. The room has exposed brick walls and a corrugated metal ceiling, with a barred window visible in the background. The lighting is low-key, casting shadows on the officers' faces and emphasizing the grim atmosphere. The scene appears to be from a film or television show.</details> | ![example16](https://media.githubusercontent.com/media/Lightricks/LTX-Video/refs/heads/main//docs/_static/ltx-video_example_00016.gif)<br><details style=\"max-width: 300px; margin: auto;\"><summary>A woman with short brown hair, wearing a maroon sleeveless top...</summary>A woman with short brown hair, wearing a maroon sleeveless top and a silver necklace, walks through a room while talking, then a woman with pink hair and a white shirt appears in the doorway and yells. The first woman walks from left to right, her expression serious; she has light skin and her eyebrows are slightly furrowed. The second woman stands in the doorway, her mouth open in a yell; she has light skin and her eyes are wide. The room is dimly lit, with a bookshelf visible in the background. The camera follows the first woman as she walks, then cuts to a close-up of the second woman's face. The scene is captured in real-life footage.</details> |\n",
    "\n",
    "#### Table of contents:\n",
    "\n",
    "- [Prerequisites](#Prerequisites)\n",
    "- [Load and run the original model](#Load-the-original-model)\n",
    "- [Convert the model to OpenVINO IR](#Convert-the-model-to-OpenVINO-IR)\n",
    "- [Compiling the model](#Compiling-the-model)\n",
    "- [Run OpenVINO model inference](#Run-OpenVINO-model-inference)\n",
    "- [Interactive inference](#Interactive-inference)\n",
    "\n",
    "### Installation Instructions\n",
    "\n",
    "This is a self-contained example that relies solely on its own code.\n",
    "\n",
    "We recommend  running the notebook in a virtual environment. You only need a Jupyter server to start.\n",
    "For details, please refer to [Installation Guide](https://github.com/openvinotoolkit/openvino_notebooks/blob/latest/README.md#-installation-guide).\n",
    "\n",
    "<img referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=5b5a4db0-7875-4bfb-bdbd-01698b5b1a77&file=notebooks/ltx-video/ltx-video.ipynb\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab89e7d-5f42-49ee-b082-eabdccd87694",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ab7ea1-a769-4904-a417-eb95bfbb8566",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "\n",
    "if not Path(\"notebook_utils.py\").exists():\n",
    "    r = requests.get(\n",
    "        url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/notebook_utils.py\",\n",
    "    )\n",
    "    open(\"notebook_utils.py\", \"w\").write(r.text)\n",
    "\n",
    "\n",
    "if not Path(\"ov_ltx_video_helper.py\").exists():\n",
    "    r = requests.get(\n",
    "        url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/notebooks/ltx-video/ov_ltx_video_helper.py\",\n",
    "    )\n",
    "    open(\"ov_ltx_video_helper.py\", \"w\").write(r.text)\n",
    "\n",
    "\n",
    "%pip install -qU \"torch>=2.1.0\" \"torchvision>=0.16\" \"diffusers>=0.28.2\" \"transformers>=4.44.2\" \"sentencepiece>=0.1.96\" \"huggingface-hub~=0.25.2\" \"einops\" \"accelerate\" \"matplotlib\" \"imageio[ffmpeg]\" \"nncf>=2.14\" \"gradio>=4.26\" --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "%pip install --pre -qU \"openvino>=2024.6.0\" --extra-index-url https://storage.openvinotoolkit.org/simple/wheels/nightly\n",
    "\n",
    "# Read more about telemetry collection at https://github.com/openvinotoolkit/openvino_notebooks?tab=readme-ov-file#-telemetry\n",
    "from notebook_utils import collect_telemetry\n",
    "\n",
    "collect_telemetry(\"ltx-video.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1ce406-e80c-4bd1-8cfa-2d8ac29cdd94",
   "metadata": {},
   "source": [
    "### Load the original model\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "152ed36b-6327-4550-aa5b-50e71a667e68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79e6dfcc0b0648629c36b20eb43d6622",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57a8694a92e84a49a17926001999b389",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import LTXPipeline\n",
    "from diffusers.utils import export_to_video\n",
    "\n",
    "pipe = LTXPipeline.from_pretrained(\"Lightricks/LTX-Video\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e25d40f-c498-427e-8f15-275ba4501777",
   "metadata": {},
   "source": [
    "### Convert the model to OpenVINO IR\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    " \n",
    "OpenVINO supports PyTorch models via conversion to OpenVINO Intermediate Representation (IR). [OpenVINO model conversion API](https://docs.openvino.ai/2024/openvino-workflow/model-preparation.html#convert-a-model-with-python-convert-model) should be used for these purposes. `ov.convert_model` function accepts original PyTorch model instance and example input for tracing and returns `ov.Model` representing this model in OpenVINO framework. Converted model can be used for saving on disk using `ov.save_model` function or directly loading on device using `core.complie_model`.\n",
    "\n",
    "`ov_ltx_video_helper.py` script contains helper function for models downloading and models conversion, please check its content if you interested in conversion details. Note that we delete the original models after conversion from pipeline to free memory.\n",
    "\n",
    "LTX Video text-to-video pipeline consists of 3 models: `Text Encoder` converts input text into embeddings, `Transformer` processes these embeddings to generate latents from noise step by step, `VAEDecoder` performs the last denoising step in conjunction with converting latents to pixels.\n",
    "\n",
    "For reducing memory consumption, weights compression optimization can be applied using [NNCF](https://github.com/openvinotoolkit/nncf). Weight compression aims to reduce the memory footprint of a model.\n",
    "models, which require extensive memory to store the weights during inference, can benefit from weight compression in the following ways:\n",
    "\n",
    "* enabling the inference of exceptionally large models that cannot be accommodated in the memory of the device;\n",
    "\n",
    "* improving the inference performance of the models by reducing the latency of the memory access when computing the operations with weights, for example, Linear layers.\n",
    "\n",
    "[Neural Network Compression Framework (NNCF)](https://github.com/openvinotoolkit/nncf) provides 4-bit / 8-bit mixed weight quantization as a compression method. The main difference between weights compression and full model quantization (post-training quantization) is that activations remain floating-point in the case of weights compression which leads to a better accuracy. In addition, weight compression is data-free and does not require a calibration dataset, making it easy to use.\n",
    "\n",
    "`nncf.compress_weights` function can be used for performing weights compression. The function accepts an OpenVINO model and other compression parameters.\n",
    "\n",
    "More details about weights compression can be found in [OpenVINO documentation](https://docs.openvino.ai/2024/openvino-workflow/model-optimization-guide/weight-compression.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c76cb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "\n",
    "to_compress_weights = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description=\"Apply Weight Compression\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "to_compress_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f819f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ov_ltx_video_helper import convert_text_encoder, convert_transformer, convert_vae_decoder\n",
    "\n",
    "\n",
    "# Uncomment line below to see model conversion code (replace to convert_transformer and convert_vae_decoder to see them too)\n",
    "# convert_text_encoder??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7f109f7-19c1-437b-bcc2-d3881e21c576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:NNCF initialized successfully. Supported frameworks detected: torch, openvino\n",
      "⌛ text_encoder conversion started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
      "/usr/lib/python3.10/importlib/util.py:247: DeprecationWarning: The `openvino.runtime` module is deprecated and will be removed in the 2026.0 release. Please replace `openvino.runtime` with `openvino`.\n",
      "  self.__spec__.loader.exec_module(self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:Statistics of the bitwidth distribution:\n",
      "┍━━━━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┑\n",
      "│ Weight compression mode   │ % all parameters (layers)   │ % ratio-defining parameters (layers)   │\n",
      "┝━━━━━━━━━━━━━━━━━━━━━━━━━━━┿━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┿━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┥\n",
      "│ int8_asym                 │ 100% (170 / 170)            │ 100% (170 / 170)                       │\n",
      "┕━━━━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┙\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9808a5221984e05924c9a11ae1d71eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ text_encoder model conversion finished\n",
      "⌛ transformer conversion started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maleksandr/test_notebooks/ltx-video/openvino_notebooks/notebooks/venv-ltx/lib/python3.10/site-packages/diffusers/models/attention_processor.py:711: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if current_length != target_length:\n",
      "/home/maleksandr/test_notebooks/ltx-video/openvino_notebooks/notebooks/venv-ltx/lib/python3.10/site-packages/diffusers/models/attention_processor.py:726: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attention_mask.shape[0] < batch_size * head_size:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:Statistics of the bitwidth distribution:\n",
      "┍━━━━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┑\n",
      "│ Weight compression mode   │ % all parameters (layers)   │ % ratio-defining parameters (layers)   │\n",
      "┝━━━━━━━━━━━━━━━━━━━━━━━━━━━┿━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┿━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┥\n",
      "│ int8_asym                 │ 100% (287 / 287)            │ 100% (287 / 287)                       │\n",
      "┕━━━━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┙\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fa1a5f42393479a884a22a8ef1a3783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ transformer model conversion finished\n",
      "⌛ vae conversion started\n",
      "INFO:nncf:Statistics of the bitwidth distribution:\n",
      "┍━━━━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┑\n",
      "│ Weight compression mode   │ % all parameters (layers)   │ % ratio-defining parameters (layers)   │\n",
      "┝━━━━━━━━━━━━━━━━━━━━━━━━━━━┿━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┿━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┥\n",
      "│ int8_asym                 │ 100% (45 / 45)              │ 100% (45 / 45)                         │\n",
      "┕━━━━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┙\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d289a1819224ebe973b27456f5dd559",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ vae model conversion finished\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "\n",
    "TEXT_ENCODER_PATH = Path(\"models/text_encoder_ir.xml\")\n",
    "TRANSFORMER_OV_PATH = Path(\"models/transformer_ir.xml\")\n",
    "VAE_DECODER_PATH = Path(\"models/vae_ir.xml\")\n",
    "\n",
    "text_encoder_dtype = pipe.text_encoder.dtype\n",
    "transformer_config = pipe.transformer.config\n",
    "vae_config = pipe.vae.config\n",
    "vae_latents_mean = pipe.vae.latents_mean\n",
    "vae_latents_std = pipe.vae.latents_std\n",
    "\n",
    "convert_text_encoder(pipe.text_encoder, TEXT_ENCODER_PATH, to_compress_weights.value)\n",
    "del pipe.text_encoder\n",
    "convert_transformer(pipe.transformer, TRANSFORMER_OV_PATH, to_compress_weights.value)\n",
    "del pipe.transformer\n",
    "convert_vae_decoder(pipe.vae, VAE_DECODER_PATH, to_compress_weights.value)\n",
    "del pipe.vae\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5b28cd-775a-4248-901d-83c7c56601bb",
   "metadata": {},
   "source": [
    "### Compiling the model\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Select device from dropdown list for running inference using OpenVINO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab2d90cc-3a12-486a-9066-1b190e59d92e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c4d58d05b1042db927e9195b2d8712a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Device:', index=1, options=('CPU', 'AUTO'), value='AUTO')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openvino as ov\n",
    "\n",
    "from notebook_utils import device_widget\n",
    "\n",
    "\n",
    "core = ov.Core()\n",
    "\n",
    "device = device_widget()\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98ab305",
   "metadata": {},
   "source": [
    "`ov_catvton_helper.py` provides wrapper classes that wrap the compiled models to keep the original interface. Note that all of wrapper classes return `torch.Tensor`s instead of `np.array`s. Then we insert wrappers instances in the pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e44184",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ov_ltx_video_helper import TextEncoderWrapper, ConvTransformerWrapper, VAEWrapper\n",
    "\n",
    "\n",
    "# Uncomment the line below to see the wrapper class code (replace to ConvTransformerWrapper and VAEWrapper to see them too)\n",
    "# TextEncoderWrapper??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e96fa82-4a2e-44a7-ae79-91a728ecdc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_transformer = core.compile_model(TRANSFORMER_OV_PATH, device.value)\n",
    "compiled_vae = core.compile_model(VAE_DECODER_PATH, device.value)\n",
    "compiled_text_encoder = core.compile_model(TEXT_ENCODER_PATH, device.value)\n",
    "\n",
    "pipe.__dict__[\"_internal_dict\"][\"_execution_device\"] = pipe._execution_device  # this is to avoid some problem that can occur in the pipeline\n",
    "\n",
    "pipe.register_modules(\n",
    "    text_encoder=TextEncoderWrapper(compiled_text_encoder, text_encoder_dtype),\n",
    "    transformer=ConvTransformerWrapper(compiled_transformer, transformer_config),\n",
    "    vae=VAEWrapper(compiled_vae, vae_config, vae_latents_mean, vae_latents_std),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c588b5-ccea-4006-acf9-42cd0482b004",
   "metadata": {},
   "source": [
    "### Run OpenVINO model inference\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2419b43",
   "metadata": {},
   "source": [
    "[General tips](https://huggingface.co/Lightricks/LTX-Video#general-tips):\n",
    "- The model works on resolutions that are divisible by 32 and number of frames that are divisible by 8 + 1 (e.g. 257). In case the input is not satisfied to the described conditions, the input will be padded with -1 and then cropped to the desired resolution and number of frames.\n",
    "- The model works best on resolutions under 720 x 1280 and number of frames below 257.\n",
    "- Prompts should be in English. The more elaborate the better. Good prompt looks like The turquoise waves crash against the dark, jagged rocks of the shore, sending white foam spraying into the air. The scene is dominated by the stark contrast between the bright blue water and the dark, almost black rocks. The water is a clear, turquoise color, and the waves are capped with white foam. The rocks are dark and jagged, and they are covered in patches of green moss. The shore is lined with lush green vegetation, including trees and bushes. In the background, there are rolling hills covered in dense forest. The sky is cloudy, and the light is dim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f504564-6357-483b-b461-b46c3b522576",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maleksandr/test_notebooks/ltx-video/openvino_notebooks/notebooks/venv-ltx/lib/python3.10/site-packages/diffusers/configuration_utils.py:140: FutureWarning: Accessing config attribute `_execution_device` directly via 'LTXPipeline' object attribute is deprecated. Please access '_execution_device' over 'LTXPipeline's config object instead, e.g. 'scheduler.config._execution_device'.\n",
      "  deprecate(\"direct config name access\", \"1.0.0\", deprecation_message, standard_warn=False)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75b2c767acc14868841116283d7c858a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {
      "test_replace": {
       "    num_frames=24,\n": "    num_frames=2,\n",
       "    num_inference_steps=40,\n": "    num_inference_steps=2,\n"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"A clear, turquoise river flows through a rocky canyon, cascading over a small waterfall and forming a pool of water at the bottom.The river is the main focus of the scene, with its clear water reflecting the surrounding trees and rocks. The canyon walls are steep and rocky, with some vegetation growing on them. The trees are mostly pine trees, with their green needles contrasting with the brown and gray rocks. The overall tone of the scene is one of peace and tranquility.\"\n",
    "negative_prompt = \"worst quality, inconsistent motion, blurry, jittery, distorted\"\n",
    "generator = torch.Generator(device=\"cpu\").manual_seed(42)\n",
    "\n",
    "\n",
    "video = pipe(\n",
    "    prompt=prompt,\n",
    "    negative_prompt=negative_prompt,\n",
    "    width=704,\n",
    "    height=480,\n",
    "    num_frames=24,\n",
    "    num_inference_steps=40,\n",
    "    generator=generator,\n",
    ").frames[0]\n",
    "export_to_video(video, \"output1_ov.mp4\", fps=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0830ee59-9257-4012-97a4-03a83b967687",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "Video(\"output1_ov.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a9ffa6-2beb-4680-b6ad-527d46160318",
   "metadata": {},
   "source": [
    "## Interactive inference\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Please select below whether you would like to use the quantized models to launch the interactive demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c9d37a4-88e2-4323-8a95-191c36f79276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "\n",
    "def generate(prompt, negative_prompt, width, height, num_frames, num_inference_steps, seed, _=gr.Progress(track_tqdm=True)):\n",
    "    generator = torch.Generator().manual_seed(seed)\n",
    "    video = pipe(\n",
    "        prompt=prompt,\n",
    "        negative_prompt=negative_prompt,\n",
    "        width=width,\n",
    "        height=height,\n",
    "        num_frames=num_frames,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        generator=generator,\n",
    "    ).frames[0]\n",
    "    file_name = \"output.mp4\"\n",
    "    export_to_video(video, file_name, fps=24)\n",
    "    return file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85299212-79bb-42fc-9173-6db09bae41a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not Path(\"gradio_helper.py\").exists():\n",
    "    r = requests.get(url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/notebooks/ltx-video/gradio_helper.py\")\n",
    "    open(\"gradio_helper.py\", \"w\").write(r.text)\n",
    "\n",
    "from gradio_helper import make_demo\n",
    "\n",
    "demo = make_demo(fn=generate)\n",
    "\n",
    "try:\n",
    "    demo.queue().launch(debug=True)\n",
    "except Exception:\n",
    "    demo.queue().launch(share=True, debug=True)\n",
    "# If you are launching remotely, specify server_name and server_port\n",
    "# EXAMPLE: `demo.launch(server_name='your server name', server_port='server port in int')`\n",
    "# To learn more please refer to the Gradio docs: https://gradio.app/docs/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "openvino_notebooks": {
   "imageUrl": "https://media.githubusercontent.com/media/Lightricks/LTX-Video/refs/heads/main//docs/_static/ltx-video_example_00003.gif?raw=true",
   "tags": {
    "categories": [
     "Model Demos",
     "AI Trends"
    ],
    "libraries": [],
    "other": [],
    "tasks": [
     "Text-to-Video"
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
